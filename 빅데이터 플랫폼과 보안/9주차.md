# Clustering

## 클러스터링 문제

클러스터링 문제는 일정한 **거리를 기준**으로 **점 집합을 여러 클러스터로 그룹화**하는 것이다. 

- 동일 클러스터 내의 점들은 서로 **가깝거나 유사**해야 하며, 다른 클러스터의 점들은 각자 서로 **달라야 한다**.
- 클러스터링은 주로 **고차원 공간**에서 수행되며, **유사도**는 다음과 같은 다양한 거리 측정 방식을 사용하여 정의된다:
  - 유클리드 거리
  - 코사인 거리
  - 자카드 거리
  - Edit Distance

### WHY is it hard?
![alt text](이미지/image-38.png)
- `클러스터링은 쉽다` 만약 우리가 고차원 공간에서 클러스터링을 한다면
  - 문제점은 차수가 점점 높아질 수록 각 클러스터간의 거리가 줄어들어 실제로는 상당히 다른 군집을 서로 가깝다고 잘못 판단할 수 있다. 
  - 심지어는 매우 `고차원의 공간`이라면 모든 클러스터에 속한 점들의 거리가 거의 같을 겄이다. 즉, `군집에 의미가 없어진다는 것`.

### Example

- 20억개의 은하를 은하의 방사선을 기준으로 7개의 차원을 가지고 그루핑을 한다고 가정하면 어떤 기준으로 은하를 분리할 수 있을까? 어떤 척도가 사용되어야 할까?
  ![alt text](이미지/image-39.png)
  
- 음원 CD 분류
  - 어떻게 음원 CD를 분류할 수 있을까?
    - 음악은 여러개의 카테고리로 나뉘고, 각 고객들은 자신이 선호하는 카테고리가 있다. 
    - 여기서, `카테고리`란 무엇일까?
  -  CD를 구매한 고객들 즉, 비슷한 선호나 취향을 가진 고객들의 집합으로 나타낼 수 있다. 그렇다면 그 반대의 경우도 마찬가지일 것이다.


## Space of all CDs

모든 CD의 공간을 생각해보자:
- 각 고객마다 하나의 차원을 가진 공간을 할당한다.
- 각 차원의 값은 0 또는 1만 가질 수 있다.
- CD는 이 공간에서 하나의 점이다 \((x_1, x_2, \ldots, x_k)\), 여기서 \(x_i = 1\)은 i번째 고객이 그 CD를 구매했음을 의미한다.

### 예시

- Amazon의 경우, 차원의 수는 수천만에 이른다.
- 우리의 과제는 비슷한 CD들의 클러스터를 찾는 것이다.
- 수천만 차원의 유사도를 계산하는것은 비효율적이고 불가하다.

## ** ★ Finding Topics ★**

#### Q. 어떻게 각 문서 혹은 단어를 수치화 할 수 있을까?
#### A. 문서 혹은 문서의 단어를 백터로 표현하는 방법!
- 문서를 벡터로 표현하는 방법을 살펴보자.
- 문서를 벡터 \((x_1, x_2, \ldots, x_k)\)로 나타낸다. 여기서 \(x_i = 1\)은 i번째 단어가 문서에 등장했음을 의미한다.

### 수식

문서 \(D\)를 벡터 \(\mathbf{v}\)로 표현할 수 있다:
\[
\mathbf{v} = (x_1, x_2, \ldots, x_k)
\]
여기서 \(x_i = 1\)은 i번째 단어가 문서에 등장했음을 의미하고, \(x_i = 0\)은 등장하지 않았음을 의미한다.

### 예시

예를 들어, 문서 \(D_1\)과 \(D_2\)가 다음과 같이 주어졌다고 가정하자:
\[
D_1 = (1, 0, 1, 0, 1)
\]
\[
D_2 = (1, 1, 0, 0, 1)
\]

### 유사성 판단

`문서` \(D_1\)과 \(D_2\)가 `비슷한 단어 집합`을 가지고 있다면, 이 문서들은 같은 `주제`에 관한 것일 가능성이 높다. 예를 들어, 두 벡터의 **내적**을 통해 유사성을 측정할 수 있다:
\[
\mathbf{v_1} \cdot \mathbf{v_2} = \sum_{i=1}^{k} x_{1i} \cdot x_{2i}
\]

### TF-IDF 개념

TF-IDF(Term Frequency-Inverse Document Frequency)는 문서 내 단어의 중요도를 측정하는 방법이다.

- **TF (Term Frequency)**:
  - **계산 방식**: 해당 단어의 빈도 / 문서의 전체 단어의 등장횟수의 총합
  - `특정 단어가 문서에 얼마나 자주 등장하는지를 나타낸다`.
  - 특정 단어가 문서 내에서 많이 등장할수록 TF 값이 높아진다.
  
  \[
  \text{TF}(t, d) = \frac{\text{Number of times term } t \text{ appears in document } d}{\text{Total number of terms in document } d}
  \]



- **IDF (Inverse Document Frequency)**:
  - 계산 방식: log(전체 문서 수 / 특정 단어가 포함된 문서 수)
  - `특정 단어가 전체 문서에서 얼마나 희귀한지 측정한다`.
  - 전체 문서에서 흔히 등장하지 않는 단어일 수록 IDF값이 높아진다.
  \[
  \text{IDF}(t, D) = \log \left( \frac{\text{Total number of documents}}{\text{Number of documents containing term } t} \right)
  \]

- **TF-IDF**:
  - TF와 IDF를 곱하여 특정 단어의 중요도를 계산한다.
  - 문서 내에서 자주 등장하면서 전체 문서에서 드물게 등장하는 단어의 TF-IDF 값이 높아진다.
  - 이를 통해 **각 문서에서 중요한 단어**를 추출하고, 문서의 주제를 파악하거나 유사도를 계산하는 데 사용할 수 있다.
  \[
  \text{TF-IDF}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t, D)
  \]

#### 의미

문서를 벡터로 표현함으로써, 단어 집합의 유사성을 기반으로 문서의 주제를 파악할 수 있다. TF-IDF를 사용하면 문서 내 단어의 중요도를 측정하여, 더 정확한 주제 파악이 가능하다.
즉, 위와 같은 방식으로 수가 아닌 대상을 **벡터형태**로 나타낸 후 군집화 할 수 있다.

## 문서를 단어의 집합으로 생각했을 때, 유사도를 측정하는 방법들
### Jacard distance
![](image-27.png)

- **자카드 유사도**는 두 집합이 얼마나 **동질**인지를 나타낸다,
- **자카드 거리**는 두 집합이 얼마나 **다른지**를 나타낸다. * 1-자카드 거리 = 자카드 유사도
- 
### Cosine Distancce

Cosine Distance는 두 벡터 사이의 내적을 나타낸다.0<= theta <=180

### 수식

- 두 벡터 \( \mathbf{x} \)와 \( \mathbf{y} \) 사이의 코사인 디스턴스는 다음과 같이 정의된다:
  \[
  \cos \theta = \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \|\mathbf{y}\|}
  \]
  여기서 \( \mathbf{x} \)와 \( \mathbf{y} \)는 벡터이다:
  \[
  \mathbf{x} = (x_1, x_2, \ldots, x_n), \quad \mathbf{y} = (y_1, y_2, \ldots, y_n)
  \]

- 두 벡터의 내적:
  \[
  \mathbf{x} \cdot \mathbf{y} = \sum_{i=1}^{n} x_i y_i
  \]

- 벡터 \( \mathbf{x} \)의 노름:
  \[
  \|\mathbf{x}\| = \sqrt{x_1^2 + x_2^2 + \ldots + x_n^2}
  \]

### 예시

벡터 \( \mathbf{x} = (1, 2, -1) \)와 \( \mathbf{y} = (2, 1, 1) \)가 주어졌다고 가정하자:

- 두 벡터의 노름:
  \[
  \|\mathbf{x}\| = \sqrt{1^2 + 2^2 + (-1)^2} = \sqrt{6}
  \]
  \[
  \|\mathbf{y}\| = \sqrt{2^2 + 1^2 + 1^2} = \sqrt{6}
  \]

- \( \mathbf{x} \)와 \( \mathbf{y} \)의 내적:
  \[
  \mathbf{x} \cdot \mathbf{y} = 1 \cdot 2 + 2 \cdot 1 + (-1) \cdot 1 = 3
  \]

- \( \mathbf{x} \)와 \( \mathbf{y} \) 사이의 각도의 코사인:
  \[
  \cos \theta = \frac{3}{\sqrt{6} \cdot \sqrt{6}} = \frac{3}{6} = \frac{1}{2}
  \]

- \( \mathbf{x} \)와 \( \mathbf{y} \) 사이의 코사인 디스턴스 (\( \theta \))는 60도이다.

##### 즉, 우리는 위에서 언급한 TF-IDF를 이용하여 문자 혹은 입력받은 데이터를 백터화 할 수 있고 코사인 유사도를 활용하여 각 벡터에 속한 요소들의 유사도를 계산할 수 있다.
  - ex) 검색엔진에서 사용자의 쿼리와 여러 문서 간의 코사인 유사도를 계산하여 관련있는 문서를 찾는데 사용한다.

### 유클리드 거리

유클리드 거리는 우리가 일반적으로 생각하는 "거리"를 측정하는 가장 익숙한 방법이다. n차원 유클리드 공간은 점들이 n개의 실수로 이루어진 벡터인 공간이다.

### 수식

벡터 \( \mathbf{x} \)와 \( \mathbf{y} \)가 주어졌을 때:
\[
\mathbf{x} = (x_1, x_2, \ldots, x_n), \quad \mathbf{y} = (y_1, y_2, \ldots, y_n)
\]

벡터 \( \mathbf{x} \)와 \( \mathbf{y} \) 사이의 유클리드 거리는 다음과 같이 정의된다:
\[
\text{Euclidean distance} = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
\]

### 예시

벡터 \( \mathbf{x} = (2, 7) \)와 \( \mathbf{y} = (6, 4) \)가 주어졌을 때:

벡터 \( \mathbf{x} \)와 \( \mathbf{y} \) 사이의 유클리드 거리는 다음과 같다:
\[
(2 - 6)^2 + (7 - 4)^2 = (-4)^2 + (3)^2 = 16 + 9 = 25 = 5
\]

따라서, 벡터 \( \mathbf{x} \)와 \( \mathbf{y} \) 사이의 유클리드 거리는 5이다.


### Edit Distance

Edit Distance는 문자열을 비교할 때 유용하다. 두 문자열 \( \mathbf{x} = x_1 x_2 \ldots x_n \)과 \( \mathbf{y} = y_1 y_2 \ldots y_m \) 사이의 거리는 \( \mathbf{x} \)를 \( \mathbf{y} \)로 `변환하는 데 필요`한 단일 문자 삽입 및 삭제의 최소 횟수로 정의된다.

### 예시

문자열 \( \mathbf{x} = \text{abcde} \)와 \( \mathbf{y} = \text{acfdeg} \)가 주어졌을 때:

Edit Distance는 3이다. 이유는 다음과 같다:
- \( \mathbf{b} \) 삭제
- \( \mathbf{f} \)를 \( \mathbf{c} \) 뒤에 삽입
- \( \mathbf{g} \)를 \( \mathbf{e} \) 뒤에 삽입

#### 문제점 및 한계
- 위에 나온 예시는 아주 짧은 문자열이기 때문에 우리가 쉽게 문자열 \(\mathbf{x}\) 를 \(\mathbf{y}\)로 변환하는데 필요한 최소한의 단일 문자 삽입 횟수를 계산할 수 있었다. 
- 하지만, 만약 문자열이 매우길고 복잡한 문자열이라면 위와 같이 계산하는게 현실적으로 불가능하고, 가능하더라도 계산한 결과가 최소의 수라는 것을 확실할 수 없다.
### 대안 

Edit Distance 를 계산하는 현실적인 식:
\[
\|\mathbf{x}\| + \|\mathbf{y}\| - 2\|\text{LCS}\|
\]

여기서 `LCS` [Longest Common Subsequence]는 두 문자열에 왼쪽에서 시작하여 오른쪽으로 한칸씩 비교해 갔을 때 등장하는 `가장 긴 공통 부분 수열`이다. (반드시 연속적일 필요는 없다.)

### 예시

문자열 \( \mathbf{x} = \text{abcde} \)와 \( \mathbf{y} = \text{acfdeg} \)에 대해, `LCS`는 \( \text{acde} \)이다.

따라서, `Edit-Distance`는 다음과 같이 계산할 수 있다:
\[
3 = 5 + 6 - 2 \times 4
\]

따라서, Edit-Distance = 3이다.

## Hamming Distance
- `해밍 거리 (Hamming Distance)`: 두 벡터 간의 **서로 다른 성분의 개수**를 의미한다.

- 해밍 거리는 주로 **불리언(Boolean) 벡터**에서 사용되며, 벡터가 `0과 1로만 구성`될 때 유용하다.
  - 원칙적으로 벡터의 성분은 어떤 집합의 값이어도 상관없다.

- **예시**: 두 벡터 10101과 11110이 주어졌을 때, 
  - 해밍 거리는 **3**이다.

# 군집화 
## 1. 계층적 군집화
![alt text](이미지/image-29.png)
### botton up method
- 각 데이터 포인트에서 시작하여 점점 더 큰 클러스터로 합쳐가는 방법.
```
1. 각 데이터 포인트를 개별 클러스터로 초기화
2. while (클러스터 수 > 1):
       a. 가장 **가까운** 두 클러스터를 찾음
       b. 두 클러스터를 하나의 클러스터로 병합
3. 병합이 완료되면 클러스터링 종료
```
### 예시: 계층적 군집화

#### 데이터 포인트
- \( A, B, C, D \)

#### 초기 상태
- \(\{A\}, \{B\}, \{C\}, \{D\}\)

#### 병합 과정
1. 가장 가까운 클러스터를 찾음 (예: \( A \)와 \( B \)):
   - 병합 후: \(\{AB\}, \{C\}, \{D\}\)

2. 다시 가장 가까운 클러스터를 찾음 (예: \( AB \)와 \( C \)):
   - 병합 후: \(\{ABC\}, \{D\}\)

3. 마지막으로 \( ABC \)와 \( D \)를 병합:
   - 최종 클러스터: \(\{ABCD\}\)

#### 클러스터 개수 선택
- 사용자가 원하는 클러스터 개수 선택
- 하나의 클러스터로 끝나는 과정은 이론적인 끝점일 뿐, 실용적인 활용에서는 사용자가 덴드로그램을 보고 **적절한 클러스터 개수를 결정**
- **즉, 사용자가 원하는 클러스터 개수를 선택하여 덴드로그램의 계층을 기준으로 클러스터링을 종료한다.**

### 덴드로그램
```
           ABCD
          /   \
        ABC    D
       / | \
     AB   C
    /  \
   A    B

```

### Top down method
- 하나의 클러스터로 시작하여 재귀적으로 분할하는 방법.
```
1. 모든 데이터 포인트를 **하나**의 클러스터로 초기화
2. while (클러스터 수 < 원하는 클러스터 수):
       a. 분할할 클러스터 선택 (보통 가장 **멀리 떨어진** 점을 찾음)
       b. 해당 클러스터를 두 개의 클러스터(의 중심점)으로 분할
       C. 새로 할당된 중심점에 가까운 점들을 새로운 클러스터에 할당
3. 분할이 완료되면 클러스터링 종료

#### 예시: Top-down 군집화
# Top-Down Clustering (Divisive Clustering) - 2차 분할

## 초기 상태
모든 데이터 포인트를 하나의 클러스터로 초기화:
`{A, B, C, D, E, F}`

## 1차 분할
가장 **멀리 떨어진** 점들 기준으로 클러스터를 둘로 나눔:
- `{A, B, C}`
- `{D, E, F}`

## 2차 분할
1. `{A, B, C}` → `{A, B}`, `{C}`
2. `{D, E, F}` → `{D, E}`, `{F}`

## 트리 형태
```
           ABCDEF
          /     \
        ABC     DEF
       / | \    / | \
     AB  C      DE  F
    / \
   A   B  ```


## Point assignment <클러스터의 중심 !!>
- 기본적으로 몇개의 클러스터 집합을 유지.
- 점은 "가장 가까운" 클러스터에 할당된다.
- 일반적으로 초기 클러스터를 초기화(만들기)위해 간단한 임의의 k개의 클러스터를 만들게 됨.
- **예시**: 초기 클러스터가 A, B, C로 설정되면, 새로운 점 D는 A, B, C 중 가장 가까운 클러스터에 할당되는 방식.
```
1. K개의 초기 클러스터 **중심을 임의**로 선택 -> 즉, 임의의 점이 클러스터의 중심이 됨
2. while (중심이 수렴하지 않음):
       a. **각** 데이터 포인트에 대해:
            i. 가장 가까운 **클러스터 중심**을 찾음
            ii. 해당 클러스터에 데이터 포인트를 할당
       b. 각 클러스터의 중심을 재계산 (할당된 점들의 평균)
3. 중심이 변하지 않거나 수렴하면 클러스터링 종료
```
### K-Means Clustering 간단한 예시 (2D 데이터)

#### 주어진 데이터
데이터:  
- \( A(1, 1), B(2, 1), C(5, 5), D(6, 6) \)  
- \( K = 2 \) (클러스터 개수: 2개)

---

#### 1. 초기화
초기 중심:  
- \( C_1 = A(1, 1) \)  
- \( C_2 = D(6, 6) \)

---

#### 2. 첫 번째 반복 (Iteration 1)

##### a. 각 데이터 포인트에 대해 가장 가까운 중심 찾기
- \( A(1, 1) \): 가까운 중심 → \( C_1(1, 1) \)
- \( B(2, 1) \): 가까운 중심 → \( C_1(1, 1) \)
- \( C(5, 5) \): 가까운 중심 → \( C_2(6, 6) \)
- \( D(6, 6) \): 가까운 중심 → \( C_2(6, 6) \)

**클러스터 할당:**  
- \( Cluster 1: \{A, B\} \)  
- \( Cluster 2: \{C, D\} \)

---

##### b. 각 클러스터 중심 재계산
- \( C_1 = \text{평균}((1, 1), (2, 1)) = (1.5, 1) \)  
- \( C_2 = \text{평균}((5, 5), (6, 6)) = (5.5, 5.5) \)

**새로운 중심:**  
- \( C_1 = (1.5, 1) \)  
- \( C_2 = (5.5, 5.5) \)

---

#### 3. 두 번째 반복 (Iteration 2)

##### a. 각 데이터 포인트에 대해 가장 가까운 중심 찾기
- \( A(1, 1) \): 가까운 중심 → \( C_1(1.5, 1) \)
- \( B(2, 1) \): 가까운 중심 → \( C_1(1.5, 1) \)
- \( C(5, 5) \): 가까운 중심 → \( C_2(5.5, 5.5) \)
- \( D(6, 6) \): 가까운 중심 → \( C_2(5.5, 5.5) \)

**클러스터 할당:**  
- \( Cluster 1: \{A, B\} \)  
- \( Cluster 2: \{C, D\} \)

##### b. 각 클러스터 중심 재계산
- \( C_1 = \text{평균}((1, 1), (2, 1)) = (1.5, 1) \)  
- \( C_2 = \text{평균}((5, 5), (6, 6)) = (5.5, 5.5) \)

**중심이 변하지 않음 → 수렴!**

---

#### 최종 결과
- **Cluster 1**: \( \{A(1, 1), B(2, 1)\} \) → 중심: \( C_1 = (1.5, 1) \)  
- **Cluster 2**: \( \{C(5, 5), D(6, 6)\} \) → 중심: \( C_2 = (5.5, 5.5) \)


## Hierarchial Clustering
![alt text](이미지/image-30.png)

#### Key operation: Repeatedly combine two nearest cluster.

### Three important question:
  1. How do you represent a cluster of more than one point?
      1) Key problem: As you merge clusters, how do you represent the “location” of each cluster, to tell which pair of clusters is closest? : 만약 우리가 강아지 웰시코기 , 닥스훈트를 분류한다고 가정할때 이미지를 백터화 했을 때 우리는 해당 백터의 요소를 유클리드 공간에서의 한 점으로 생각할 수 없다. 즉, 한마디로 다른 **척도**를 사용해야 한다.
   
      2) **Euclidean case**: each cluster has a centroid =
average of its (data)points : 유클리디안 공간에 있는 점 들의 중심<평균값>을 `centroid`라고 정의한다.

  1. How do you determine the “nearness” of clusters?
     1. Measure cluster distances by distances of centroids
     - 각 점과 점끼리 , 점과 클러스터 끼리 가까움(거리)는 직관적이고 쉽게 측정할 수 있다.
     - 그러나 클러스터와 클러스터 끼리의 **가까움(거리?)**는 어떻게 정의할지 애매하다 
  2. When to stop combining clusters?
  
![alt text](이미지/image-31.png)




![alt text](이미지/image-32.png)

### Efficiency in Hierarchical Clustering

#### Naïve Implementation of Hierarchical Clustering <간단 무식한 방법>
#### 첫 번째 단계에서 쌍의 개수
- 초기에는 **N개의 클러스터**가 있다. 각 클러스터 쌍의 거리를 계산해야 하므로, 모든 클러스터 쌍의 개수를 구한다.
- 중복을 피하기 위해, 두 클러스터 간의 쌍을 조합(combination)으로 계산한다.
- **총 쌍의 개수**는 \( \binom{N}{2} = \frac{N \times (N-1)}{2} \)이다.
- 따라서 첫 번째 단계에서의 시간 복잡도는 **\( O(N^2) \)**이다.

#### 단계별 클러스터 쌍 계산
- 클러스터가 한 쌍씩 병합되므로, 각 단계마다 클러스터 수가 줄어든다.
- 두 번째 단계에서는 \( N-1 \)개의 클러스터가 남아 있으므로, 쌍의 개수는 \( \frac{(N-1) \times (N-2)}{2} \)가 된다.
- 이와 같은 방식으로 클러스터가 1개가 될 때까지 반복한다.
- 각 단계의 쌍의 개수는 \( N, N-1, N-2, \dots, 1 \)로 감소한다.

##### 전체 시간 복잡도 계산
- 각 단계에서의 시간 복잡도를 모두 더한 총 계산량은 다음과 같다:
  \[
  O(N^2) + O((N-1)^2) + O((N-2)^2) + \dots + O(1^2)
  \]
- 이 합의 계산 결과는 \( O(N^3) \)이다. 이를 수학적으로 표현하면:
  \[
  \sum_{k=1}^{N} k^2 = \frac{N(N+1)(2N+1)}{6} \approx O(N^3)
  \]

##### 결론
- 기본 구현 방식의 전체 시간 복잡도는 \( O(N^3) \)이다.
- 각 단계에서 모든 클러스터 쌍 간 거리를 다시 계산하기 때문에 매우 비효율적이다.
- 따라서, 대규모 데이터셋에는 부적합하다.

#### 대안: 우선순위 큐를 사용 한다. 
- **우선순위 큐**를 사용하여 효율적으로 클러스터 쌍을 선택하면 시간 복잡도를 줄일 수 있다.
- **개선된 시간 복잡도**: \(O(N^2 \log N)\)
- **한계**: 여전히 메모리에 맞지 않는 **대규모 데이터셋**에는 너무 비효율적.
- 우선순위 큐 max_heap , min_heap를 사용하면서 가장 큰 값과 가장 작은 값을 한번에 찾기 때문에 사용한다.
- 우선순위 큐에 들어가는 값이 거리값 `N^2개`(처음에 거리값을 계산해야하기 때문에)을 저장 -> 가장 작은 거리를 바로 상수시간에 찾을 수 있음 -> 기존 배열에서 가장 작은 거리 값을 제거했기 때문에 밑의 나머지 배열을 다시 **heapify**해야하는데 이게  logN의 시간을 소요함


## 비유클리드 공간에서의 클러스터링

### 비유클리드 공간에서의 위치 개념
- 비유클리드 공간에서는 클러스터의 위치를 나타내기 위해 사용할 수 있는 것은 개별 데이터 포인트뿐이다. 즉 (x,y,z)좌표계가 존재하지 않는다.
- 각 점의 좌표가 존재 하지 않으므로 `평균` 개념이 존재하지 않는다. 즉, 두 점의 "평균"을 계산할 수 없다.

### 접근 방법
1. **클러스터의 표현 방식**
   - Clustroid를 사용한다. Clustroid는 클러스터 내 다른 점들과 `가장 가까운` `데이터 포인트`로 정의된다.
     - 이 때 `Centroid`는 각 좌표값들의 평균값을 갖는 점이므로 실제로는 존재하지 않는 좌표이다. 반면
     - `Clusteroid` 는 실제하는 값이다.

2. **클러스터 간 거리 측정**
   - Clustroid를 중심점(Centroid)처럼 취급하고, 클러스터 간 거리 계산 시 Clustroid 간 거리를 사용한다.

![alt text](이미지/image-33.png)

#### "가장 가까운" 점(Clustroid) ??

#### 클러스터를 표현하는 방법
- Clustroid는 클러스터 내 다른 점들과 `"가장 가까운"` 점으로 정의된다.

##### Clustroid를 선택하는 기준
- Clustroid를 선택할 때, "가장 가까움"의 의미를 다음과 같은 기준 중 하나로 `정의`한다.
  - 다른 점들과의 **최대 거리**가 가장 작은 점.
  - 다른 점들과의 **거리 합**이 가장 작은 점.
  - 다른 점들과의 **거리 제곱합**이 가장 작은 점.
    - 예를 들어, 거리 측정 지표 \( d \)에 대해, 클러스터 \( C \)의 clustroid \( c \)는 다음 식으로 표현된다: 
      \[
      \min_c \sum_{x \in C} d(x, c)^2
      \]

##### Centroid와 Clustroid의 차이
- **Centroid**는 클러스터 내 모든 점의 평균값으로, 인위적인 점이다.
- **Clustroid**는 실제 존재하는 데이터 포인트 중에서 클러스터 내 다른 모든 점들과 "가장 가까운" 점이다.

![alt text](이미지/image-34.png)


### 클러스터 간 "가까움" 정의

#### 클러스터 간 거리 측정 방법
1. **최소 거리 (Min Distance)**
   - 각 클러스터에서 한 점씩 선택했을 때, 두 점 간 거리가 가장 짧은 값을 사용한다.

2. **최대 거리 (Max Distance) 또는 지름 (Diameter)**
   - 클러스터 내 모든 점 쌍 사이의 거리 중 가장 큰 값을 사용한다. 병합된 클러스터의 지름을 사용한다.

3. **반경 (Radius)**
   - 클러스터의 중심점과 클러스터 내 모든 점 사이의 거리 중 최대값<지름>을 사용한다.

1. **평균 거리 (Group Average Distance)**
   - 두 클러스터 사이의 거리를 각 클러스터에서 선택된 모든 점 쌍의 **평균 거리**로 정의한다.

2. **중심 거리 (Centroid Distance)**
   - 각 클러스터의 중심점(centroid) 간의 거리로 측정한다.

3. **밀도 기반 접근법 (Density-Based Approach)**
   - 클러스터의 **지름** 또는 **평균 거리**를 계산한 후, 클러스터 내 점의 개수로 나눠 밀도를 고려한 거리를 정의한다.

![alt text](이미지/image-35.png)

# k-Means

- 유클리디안 공간을 가정한다.
- 처음에 k 값(클러스터의)수를 정하는 것에서 시작한다.
- 즉, k개의 centroid를 기준으로 클러스터링을 하겠다는 이야기
```
# k-means 알고리즘 의사코드

1. **입력**: 데이터 집합 \( X \), 클러스터 수 \( k \)
2. **출력**: 각 데이터 포인트의 클러스터 할당 및 각 클러스터의 중심점

3. **초기화**:
   - **방법 1**:
     1. `무작위`로 `첫 번째 점`을 선택하여 첫 번째 클러스터의 중심으로 지정 -> 즉 centroid역할을 해 줄 점을 찾아야 함.
     2. 나머지 \( k-1 \)개의 점을 선택할 때, 이전에 선택한 점들로부터 가장 멀리 떨어진 점을 하나씩 선택하여 중심으로 지정

   - **방법 2**:
     1. 일정한 데이터 샘플을 사용하여 **계층적 군집화**를 사용한 후 \( k \)개의 초기 클러스터 생성 -> 방법1에서 발생하는 전혀 상관없는  centroid를 만드는 것을 방지
     2. 각 클러스터에서 중심에 가장 가까운 점을 초기 클러스터 중심으로 지정

4. **반복**:
   - 모든 데이터 포인트에 대해:
     - 각 포인트를 가장 가까운 클러스터 중심에 할당
   - 각 클러스터에 대해:
     - 클러스터에 속한 점들의 평균을 계산하여 새로운 중심으로 설정

5. **종료 조건**: 중심의 위치가 더 이상 변하지 않거나 설정된 반복 횟수에 도달하면 종료

6. **결과 반환**: 각 데이터 포인트의 클러스터 할당과 최종 클러스터 중심


```

![alt text](이미지/image-36.png)

![alt text](이미지/image-37.png)


### 최적의 K값을 구하기
- 각각다른 k값을 가지고 클러스터링을 실행했을 때, 센트로이드와 나머지 점들에 대한 평균 거리가 k값이 작다면 크고 , k값이 클 수록 줄어든다.
- 만약 적합한 k점 이후 k값이 증가하더라도 평균 거리값이 매우 작게 줄어들게 된다.
![alt text](이미지/image-40.png)
- 만약 k값이 계속 증가하게 된다면 차수가 증가하므로 복잡도가 증가하게 되어, 적합한 k값을 찾는게 중요!

### K-means 의 장-단 점
#### 장점
- 구현이 쉽다.
- k값이 작다면 k-평균이 계층적 군집화보다 계산 속도가 빠를 수 있다.
- k-평균이 계층적 군집화보다 더 밀집된 클러스터를 생성할 수 있다.
- 인스턴스가 클러스터를 변경할 수 있다 -> 중심(centroid)가 재 할당된다.

#### 단점
- 클러스터의 수(k값)를 예측하기 어렵다.
- 초기 시드가 최종 결과에 큰 영향을 미친다. -> 2번째 방식을 사용하는 이유
- 구형 클러스터에만 잘 작동한다. -> 즉, 구형 군집에 대해서만 좋은 성능을 발휘한다.
- 이상치에 매우 민감하다. -> 이상치가 centroid에 큰 영향을 주므로 centroid 기반의 알고리즘인 k-means는 이상치에 매우 민감하다.

### 계층적 군집화 장-단점
#### 장점
- k-평균이 반환하는 평면 클러스터보다 더 많은 정보를 제공한다.
- 덴드로그램을 통해 클러스터의 수를 결정하기 쉽다.
- 구현이 쉽다. -> k개의 시작점 혹은 군집을 미리 초기화 할 필요 없다.

#### 단점
- 이전 단계를 되돌릴 수 없다.
- 인스턴스가 클러스터에 할당되면 더 이상 이동할 수 없다.
- 시간 복잡도가 높아 대규모 데이터셋에 적합하지 않다.
- 초기 시드가 최종 결과에 큰 영향을 미친다.
- 데이터의 순서가 최종 결과에 영향을 미친다.
- 이상치에 매우 민감하다.
