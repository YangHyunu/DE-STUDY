##  Docker-Compose 를 사용해서 Hadoop_Cluster 빌드하기

### 1. yaml 파일 작성
```
version: "3"
services:
  namenode:
    image: apache/hadoop:3 # 아파치 하둡3를 사용
    hostname: namenode
    volumes:
      - ./Makefile:/opt/hadoop/Makefile
    ports:
      - 9870:9870  # Namenode 웹 UI 포트
    env_file:
      - ./config
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
    command: ["hdfs", "namenode"]
  
  datanode_1:
    image: apache/hadoop:3
    command: [ "hdfs", "datanode" ]
    env_file:
      - ./config

  datanode_2:
    image: apache/hadoop:3
    command: [ "hdfs", "datanode" ]
    env_file:
      - ./config

  resourcemanager:
    image: apache/hadoop:3
    hostname: resourcemanager 
    command: [ "yarn", "resourcemanager" ]
    ports:
      - 8088:8088  # ResourceManager 웹 UI 포트
    env_file:
      - ./config
    volumes:
      - ./test.sh:/opt/test.sh

  nodemanager:
    image: apache/hadoop:3
    command: [ "yarn", "nodemanager" ]
    env_file:
      - ./config

```
- `Application manager`는 리소스 매니저가 실행될때 YARN 시스템 내부에서 자동으로 실행됨 
- 즉, YARN 클러스터가 설정되면, 각 애플리케이션이 제출될 때마다 Application Master는 ResourceManager와 협력하여 애플리케이션 실행을 관리
- 로컬 브라우저나 ssh로 접속하여 해당 포트의 `웹 UI에 접속`할 수 있음 



### Docker-Compose 방식의 장점:
  1.  환경 설정의 간편함
  2. 다중 노드 클러스터 테스트의 용이
  3. 환경의 일관성 유지
  4. 의존성 문제 해결
Hadoop
Spark
Hive
Jeppin은 안쓸예정

## config 
```
HADOOP_HOME=/opt/hadoop
CORE-SITE.XML_fs.default.name=hdfs://namenode
CORE-SITE.XML_fs.defaultFS=hdfs://namenode
HDFS-SITE.XML_dfs.namenode.rpc-address=namenode:8020
HDFS-SITE.XML_dfs.replication=3
MAPRED-SITE.XML_mapreduce.framework.name=yarn
MAPRED-SITE.XML_yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=$HADOOP_HOME
MAPRED-SITE.XML_mapreduce.map.env=HADOOP_MAPRED_HOME=$HADOOP_HOME
MAPRED-SITE.XML_mapreduce.reduce.env=HADOOP_MAPRED_HOME=$HADOOP_HOME
YARN-SITE.XML_yarn.resourcemanager.hostname=resourcemanager
YARN-SITE.XML_yarn.nodemanager.pmem-check-enabled=false
YARN-SITE.XML_yarn.nodemanager.delete.debug-delay-sec=600
YARN-SITE.XML_yarn.nodemanager.vmem-check-enabled=false
YARN-SITE.XML_yarn.nodemanager.aux-services=mapreduce_shuffle
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.maximum-applications=10000
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.maximum-am-resource-percent=0.1
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.resource-calculator=org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.queues=default
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.capacity=100
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.user-limit-factor=1
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.maximum-capacity=100
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.state=RUNNING
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.acl_submit_applications=*
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.acl_administer_queue=*
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.node-locality-delay=40
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.queue-mappings=
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.queue-mappings-override.enable=false

```
- HADOOP_HOME=/opt/hadoop
Hadoop 설치 경로를 설정합니다.
- CORE-SITE.XML_fs.default.name 및 CORE-SITE.XML_fs.defaultFS
HDFS의 기본 파일 시스템을 hdfs://namenode로 지정합니다.
- HDFS-SITE.XML_dfs.namenode.rpc-address
이름노드의 RPC 주소를 설정합니다 (namenode:8020).
- HDFS-SITE.XML_dfs.replication
HDFS 데이터의 복제 수를 3로 설정합니다.
- MAPRED-SITE.XML_mapreduce.framework.name
MapReduce의 실행 프레임워크를 YARN으로 설정합니다.
- YARN-SITE.XML_yarn.resourcemanager.hostname
YARN 리소스 관리자 호스트를 resourcemanager로 지정합니다.
- YARN-SITE.XML_yarn.nodemanager.pmem-check-enabled 및 YARN-SITE.XML_yarn.nodemanager.vmem-check-enabled
YARN의 노드 관리자가 물리 메모리와 가상 메모리 체크를 비활성화합니다.
- CAPACITY-SCHEDULER.XML
YARN의 캐패시티 스케줄러 설정입니다. 여러 앱의 자원 할당과 큐에 대한 다양한 설정을 관리합니다.

#### 주요 기능 요약:
- `HDFS 구성`: namenode와 두 개의 datanode를 통해 분산 파일 시스템을 구성.

- `YARN 구성`: resourcemanager와 nodemanager가 YARN 클러스터를 구성.

- `SSH 터널링`: SSH 터널링을 사용하여 로컬 컴퓨터에서 Hadoop 및 YARN 웹 UI에 안전하게 접근할 수 있다.

- `환경 변수`: Hadoop의 주요 설정 값들을 Docker 컨테이너에서 쉽게 사용할 수 있도록 환경 변수를 지정.



## Docker Compose 이미지 빌드하기 
```
docker-compose up -d
```

```
[+] Running 27/5
 ✔ nodemanager Pulled                                                           60.8s 
 ✔ datanode_1 Pulled                                                            60.8s 
 ✔ datanode_2 Pulled                                                            60.8s 
 ✔ namenode Pulled                                                              60.8s 
 ✔ resourcemanager Pulled                                                       60.8s 
[+] Running 6/6
 ✔ Network hadoop_cluster_default              Created                           0.1s 
 ✔ Container hadoop_cluster-resourcemanager-1  Started                          59.9s 
 ✔ Container hadoop_cluster-datanode_2-1       Started                          59.6s 
 ✔ Container hadoop_cluster-datanode_1-1       Started                          59.9s 
 ✔ Container hadoop_cluster-nodemanager-1      Started                          59.7s 
 ✔ Container hadoop_cluster-namenode-1         Started                          59.7s 
```
도커 컴포즈가 정상적으로 빌드 됐으니 테스트 해보자 !

## SSH 개념 & Docker-Compose와의 관련성




##### SSH 터널링 방식으로 접근 (vm을 사용할떄)
도커 컨테이너 id를 확인할 수 있다.
```
ssh -L 9870:<컨테이너 ip>user@<remote-host의 ip>
ssh -L 8088:<컨테이너 ip>:8088 user@remote-host
```
위 명령어를 사용하여 SSH 터널링을 사용하여 로컬 컴퓨터에서 Hadoop 및 YARN 웹 UI에 안전하게 접근할 수 있고, 로컬 브라우저에서 해당 URL로 웹UI에 접속할 수 있다!

컨테이너 IP 확인
```
docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' <컨테이너 아이디>
```
내 컴퓨터 IP 확인
```
ipconfig
```

#### 로컬에서 접근

##### Hadoop NameNode UI
```
http://localhost:8088/
```
로 접근 하면 
![alt text](image.png)

##### YARN ResourceManager UI
```
localhost/9870
```
![alt text](image-1.png)

## Hadoop 클러스터를 활용해 데이터를 저장하거나 분산 처리작업 실행하기.
1. Sample Job submit
  - Yarn 커맨드를 사용해서 네임노드에 MR작업 보내기
```
# 네임노드에 접속
docker exec -ti hadoop_cluster-namenode-1 /bin/bash
ls /opt/hadoop/share/hadoop/mapreduce/
#위 커맨드를 입력하여 예제 JAR파일의 실제 버전을 확인
# yarn 커맨드를 사용하여 pi 값을 구하는 예제를 실행
 yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar pi 10 15
```
##### 결과
```
bash-4.2$ yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar pi 10 15
Number of Maps  = 10
Samples per Map = 15
Wrote input for Map #0
Wrote input for Map #1
Wrote input for Map #2
Wrote input for Map #3
Wrote input for Map #4
Wrote input for Map #5
Wrote input for Map #6
Wrote input for Map #7
Wrote input for Map #8
Wrote input for Map #9
Starting Job
2024-10-17 01:07:51 INFO  DefaultNoHARMFailoverProxyProvider:64 - Connecting to ResourceManager at resourcemanager/172.18.0.5:8032
2024-10-17 01:07:52 INFO  JobResourceUploader:907 - Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1729126200903_0001
2024-10-17 01:07:53 INFO  FileInputFormat:300 - Total input files to process : 10
2024-10-17 01:07:53 INFO  JobSubmitter:202 - number of splits:10
2024-10-17 01:07:53 INFO  JobSubmitter:298 - Submitting tokens for job: job_1729126200903_0001
2024-10-17 01:07:53 INFO  JobSubmitter:299 - Executing with tokens: []
2024-10-17 01:07:53 INFO  Configuration:2854 - resource-types.xml not found
2024-10-17 01:07:53 INFO  ResourceUtils:476 - Unable to find 'resource-types.xml'.
2024-10-17 01:07:54 INFO  YarnClientImpl:338 - Submitted application application_1729126200903_0001
2024-10-17 01:07:54 INFO  Job:1682 - The url to track the job: http://resourcemanager:8088/proxy/application_1729126200903_0001/
2024-10-17 01:07:54 INFO  Job:1727 - Running job: job_1729126200903_0001
2024-10-17 01:08:07 INFO  Job:1748 - Job job_1729126200903_0001 running in uber mode : false
2024-10-17 01:08:07 INFO  Job:1755 -  map 0% reduce 0%
2024-10-17 01:08:16 INFO  Job:1755 -  map 10% reduce 0%
2024-10-17 01:08:18 INFO  Job:1755 -  map 20% reduce 0%
2024-10-17 01:08:19 INFO  Job:1755 -  map 30% reduce 0%
2024-10-17 01:08:20 INFO  Job:1755 -  map 40% reduce 0%
2024-10-17 01:08:21 INFO  Job:1755 -  map 50% reduce 0%
2024-10-17 01:08:22 INFO  Job:1755 -  map 60% reduce 0%
2024-10-17 01:08:24 INFO  Job:1755 -  map 70% reduce 0%
2024-10-17 01:08:25 INFO  Job:1755 -  map 80% reduce 0%
2024-10-17 01:08:26 INFO  Job:1755 -  map 90% reduce 0%
2024-10-17 01:08:28 INFO  Job:1755 -  map 100% reduce 0%
2024-10-17 01:08:30 INFO  Job:1755 -  map 100% reduce 100%
2024-10-17 01:08:31 INFO  Job:1766 - Job job_1729126200903_0001 completed successfully
2024-10-17 01:08:31 INFO  Job:1773 - Counters: 54
        File System Counters
                FILE: Number of bytes read=226
                FILE: Number of bytes written=3045185
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=2600
                HDFS: Number of bytes written=215
                HDFS: Number of read operations=45
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=3
                HDFS: Number of bytes read erasure-coded=0
        Job Counters
                Launched map tasks=10
                Launched reduce tasks=1
                Rack-local map tasks=10
                Total time spent by all maps in occupied slots (ms)=54086
                Total time spent by all reduces in occupied slots (ms)=9363
                Total time spent by all map tasks (ms)=54086
                Total time spent by all reduce tasks (ms)=9363
                Total vcore-milliseconds taken by all map tasks=54086
                Total vcore-milliseconds taken by all reduce tasks=9363
                Total megabyte-milliseconds taken by all map tasks=55384064
                Total megabyte-milliseconds taken by all reduce tasks=9587712
        Map-Reduce Framework
                Map input records=10
                Map output records=20
                Map output bytes=180
                Map output materialized bytes=280
                Input split bytes=1420
                Combine input records=0
                Combine output records=0
                Reduce input groups=2
                Reduce shuffle bytes=280
                Reduce input records=20
                Reduce output records=0
                Spilled Records=40
                Shuffled Maps =10
                Failed Shuffles=0
                Merged Map outputs=10
                GC time elapsed (ms)=1234
                CPU time spent (ms)=10470
                Physical memory (bytes) snapshot=3069726720
                Virtual memory (bytes) snapshot=29543190528
                Total committed heap usage (bytes)=2535981056
                Peak Map Physical memory (bytes)=304001024
                Peak Map Virtual memory (bytes)=2686943232
                Peak Reduce Physical memory (bytes)=253435904
                Peak Reduce Virtual memory (bytes)=2691653632
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters
                Bytes Read=1180
        File Output Format Counters
                Bytes Written=97
Job Finished in 40.098 seconds
Estimated value of Pi is 3.17333333333333333333
```
#### 결과 설명

##### `Number of Maps and Samples per Map`:
```
Number of Maps  = 10
Samples per Map = 15
```
- Map 작업 개수: 10개의 Map 작업이 병렬로 실행되며 각 Map 작업은 15개의 샘플을 처리하게됨 즉,
 총 150개의 샘플이 처리됨.

##### - `Job Submission and ResourceManager`:
```Connecting to ResourceManager at resourcemanager/172.18.0.5:8032```
- ResourceManager 연결: YARN의 ResourceManager는 자원을 관리하며,MapReduce 작업이 제출되면 자원을 할당.

##### `Job Status`:
```
map 0% reduce 0%
map 100% reduce 100%
Job job_1729126200903_0001 completed successfully
```
- Map과 Reduce 작업 상태: Map 작업과 Reduce 작업의 진행 상황이 출력되며, 100% 완료되면 작업이 성공적으로 완료되었음을 표시함.

##### `File System Counters`:
```
HDFS: Number of bytes read=2600
HDFS: Number of bytes written=215
```
- HDFS 관련 카운터: 이 작업에서 HDFS에서 읽은 데이터와 쓴 데이터의 크기를 보여줌

##### `Map-Reduce Framework Counters`:
```
Launched map tasks=10
Launched reduce tasks=1
Map input records=10
Map output records=20
Reduce input records=20
Reduce output records=0
```
- Map과 Reduce 작업 통계: Map 작업 10개, Reduce 작업 1개가 실행되었으며, Map 작업은 10개의 입력 레코드를 처리해 20개의 중간 결과를 출력했음. Reduce 작업에서는 20개의 입력 레코드를 받아 최종 결과를 생성했다.

##### `Shuffle 및 메모리 사용량`:
```
Shuffled Maps =10
Peak Map Physical memory (bytes)=304001024
Peak Reduce Physical memory (bytes)=253435904
```
- Shuffle 단계: Shuffle 단계에서는 Map 작업의 결과를 Reduce 작업으로 전달. M
- 메모리 사용량: 각 Map과 Reduce 작업에서 사용된 물리적 메모리와 가상 메모리의 피크 사용량이 기록됨.

##### `작업 완료 시간 및 Pi 근사값`:
```
Job Finished in 40.098 seconds
Estimated value of Pi is 3.17333333333333333333
```
작업이 40초 내에 완료되었고, Pi의 근사값은 3.1733333333333333으로 계산되었음을 출력함.
