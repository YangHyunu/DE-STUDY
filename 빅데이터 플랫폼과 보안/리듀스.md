# 2주차 MapReduce

대기업에서는 현재 MapReduce에 큰 관심을 두지 않고, 데이터 엔지니어링에 더 집중하고 있다. 병렬 혹은 분산 처리를 위한 프로그래밍은 매우 어렵고, 순차적인 프로세싱에 싱크가 맞지 않는 등의 문제가 발생한다. 이는 많은 데이터를 읽고 쓰고 저장하고 관리하는 과정에서 나타나는 문제들이다. 이런 문제를 해결하기 위해 **분산 파일 시스템(DFS)**이 도입되었으며, 데이터를 잘게 쪼개어 저장하고, 연산은 데이터를 직접 처리할 수 있는 위치에서 수행하는 방식이 사용된다.

## 핵심 개념

- 병렬처리와 대규모 데이터 처리를 자동화하는 인터페이스와, 이를 저렴한 상용 PC들로 구성된 클러스터에서 고성능으로 실행할 수 있는 플랫폼이 요구된다.
- **Scale up**: 한 대의 컴퓨터 성능을 업그레이드.
- **Scale out**: 여러 대의 컴퓨터를 활용해 성능을 향상시킴. 가성비 측면에서 유리하다.

## 문제 상황

- 200억 개 이상의 웹페이지를 저장하려면 약 400TB 이상의 용량이 필요하다. 이를 한 대의 컴퓨터가 처리하려면 4개월 이상이 걸린다. 분산 처리가 필요한 이유이다.

## 대규모 컴퓨팅의 과제

- **컴퓨팅 분배**: 어떻게 계산 작업을 분배할 것인가?
- **분산 프로그램 작성**: 분산 프로그램을 쉽게 작성하는 방법은 무엇인가?
- **장애 처리**: 서버가 지속적으로 고장날 때 이를 어떻게 처리할 것인가? 대규모 클러스터에서는 매일 서버가 고장날 가능성이 크다.
- **네트워크 문제**:
  - 데이터를 네트워크로 복사하는데 시간이 오래 걸림.
  - 좋은 네트워크 장비는 비용이 많이 든다.

## 핵심 아이디어

- **데이터 근처에서 연산 수행**: 데이터를 네트워크로 옮기지 않고 데이터가 있는 위치에서 직접 처리하면 시간을 절약할 수 있다.
- **파일 복제(Replication)**: 데이터를 여러 개의 복사본으로 저장하여, 한 복사본이 손상되면 다른 복사본으로 복구한다.

## MapReduce 개요

MapReduce는 위 문제들을 해결하는 분산 처리 모델이다. 데이터를 작은 단위로 나누어(Map) 처리한 후, 결과를 모아(Reduce) 최종 결과를 생성하는 방식이다.

## 저장 인프라

- **Google GFS (Google File System)**
- **Hadoop HDFS (Hadoop Distributed File System)**


### 전형적인 사용 패턴 (Typical Usage Pattern)

- **Huge files**: 일반적으로 수백 GB에서 수 TB에 이르는 대용량 파일을 처리한다.
- **WORM (Write Once, Read Many)**: 한 번 기록한 후 여러 번 읽기가 일반적인 패턴이다.
  - 데이터는 거의 업데이트되지 않는다. 즉, 한 번 기록된 데이터는 자주 변경되지 않으며, 필요한 경우 새로운 데이터를 추가하는 방식으로 처리한다.
  - 읽기와 추가 작업이 주로 이루어진다.

## 분산 파일 시스템(DFS)

- **네임노드(NameNode) or 마스터 노드**: 클러스터에 저장된 `네임스페이스`와 `메타데이터`를 관리하며, 파일의 청크 수와 위치 정보를 제공.

- **데이터 노드 or 청크서버(Chunk Servers)**: 파일을 작은 청크(16-128MB)로 나누어 저장하며, 각 청크는 보통 2-3번 복제되어 안전성을 확보.

## 클라이언트와의 통신

- 클라이언트는 네임노드와 통신하여 청크 서버의 메타데이터(파일 위치 및 정보)를 받는다.
- 이후 클라이언트는 청크 서버에 직접 연결하여 데이터를 액세스한다.

MapReduce는 대용량 데이터를 병렬로 처리하는 효율적인 방법을 제공하며, 분산 파일 시스템을 통해 데이터를 안전하게 저장하고 관리한다.

![alt text](<image (3).png>)


## Programming Model: MapReduce

### Warm-up task:

- **목적**: 큰 텍스트 문서에서 각 `단어`가 `몇 번` 나타나는지 세기
    - 이는 단순 텍스트가 아니라 웹사이트의 방문자 수와 같은 다른 데이터일 수도 있음
- 파일에서 고유한 단어가 등장하는 빈도를 카운트하는 작업

### Sample Application:

- **적용 예시**: 웹 서버 로그 분석을 통해 인기 있는 URL 찾기

### Word Count Example:

- **Count occurrences of words**:
    - `words(doc.txt) | sort | uniq -c`
        - `words` 함수는 파일을 읽어 각 단어를 한 줄씩 출력
        - 그 후 단어 목록을 알파벳순으로 정렬하고, 중복된 단어를 제거하며 등장 횟수를 계산

### MapReduce의 핵심:

- 이 프로세스는 **MapReduce**의 본질그자체를 나타냄:
    - 가장 큰 **장점**은 자연스럽게 병렬화가 가능하다는 것
    - 즉, 각 단계가 **독립적으로 실행**되기 때문에 대용량 데이터를 병렬로 처리할 수 있음

---

1. **`words(doc.txt)`**: `doc.txt` 파일에서 각 단어를 한 줄씩 출력
2. **`sort`**: 첫 번째 명령어의 출력(단어 목록)을 입력으로 받아 단어를 알파벳순으로 정렬
3. **`uniq -c`**: 정렬된 결과를 받아 중복된 단어를 제거하고, 각 단어의 빈도를 계산하여 출력

따라서 **`|` (파이프)**는 각 명령어의 출력을 다음 명령어의 입력으로 연결해 작업을 연속적으로 처리하게 만듦. 이 과정은 서로 **Dependency(의존성)**가 없기 때문에 대용량 데이터를 **병렬 처리**할 수 있다는 점에서 매우 유용함.

## MapReduce: Overall Procedures

1. **Sequentially read a lot of data**
    - 데이터를 순차적으로 읽어들임
2. **Map**:
    - 처리하고자 하는<관심있는> 데이터를 추출함
3. **Group by key**:
    - 데이터를 키로 그룹화하고 정렬 및 셔플을 수행함
4. **Reduce**:
    - 데이터를 집계, 요약, 필터 또는 변환함
5. **Write the result**:
    - 결과를 기록함

# MapReduce Example: 중앙대학교 문서 처리

## 1. Map 단계:
![alt text](image-4.png)
### 입력 데이터:
- **Key**: 중앙대학교와 관련된 문서 파일명
- **Value**: 문서 내용 전체

### Map 함수 적용 후:
- 문서를 한 줄씩 읽어들여 Key와 Value를 변환.
- **변경된 Key**: "중앙대학교" (특정 단어 또는 토픽)
- **변경된 Value**: 해당 줄에서 "중앙대학교"가 등장하는 빈도수

즉, 문서 파일명(Key)과 내용(Value)이 Map 함수를 거친 후, **"중앙대학교"**라는 단어(Key)와 그 단어의 빈도수(Value)로 변환됨.

## 2. Group by Key 단계:

Group by Key 단계는 **솔트(Salt)**와 **셔플(Shuffle)** 과정을 포함하여, 데이터를 올바르게 그룹화하고 병렬 처리를 가능하게 한다.

### 솔트(Salt):
- 솔트는 동일한 키 값이 한 리듀서(Reducer)에 집중되는 것을 방지하기 위해 키 값에 랜덤한 값을 추가하는 과정이다.
- 예를 들어, "중앙대학교"라는 키가 너무 많을 경우, 동일한 키를 여러 리듀서로 분산시키기 위해 일부 키에 중앙대학교_1,중앙대학교_2 와 같은 임의로 할당하여 변형시킨다.
  ![alt text](image-6.png)
- 이를 통해 처리 부하가 분산되고 성능을 향상시킬 수 있다.

### 셔플(Shuffle):
- 셔플은 Map 단계에서 생성된 Key-Value 쌍을 적절한 리듀서로 전송하는 과정이다.
- 같은 키 값을 가진 데이터는 셔플 단계에서 한곳으로 모아 동일한 리듀서가 처리할 수 있도록 한다.
- 예를 들어, "중앙대학교"라는 키를 가진 데이터가 여러 문서에 분포되어 있어도, 셔플 과정을 통해 하나의 리듀서에서 그룹화된다.

### 결과:
- **그룹화된 데이터**:
  - "중앙대학교" → 문서 1에서 3번, 문서 2에서 2번, 문서 3에서 1번 등장
- 이 단계에서는 `Key는 그대로 유지`되고, Value는 그룹화된 데이터 집합이다.

## 3. Reduce 단계:
![alt text](image-5.png)
- Group by Key에서 그룹화된 데이터를 받아 Reduce 함수를 적용.
- **Reduce 함수**는 동일한 Key를 가진 Value들을 집계하는 작업을 수행.
- 예를 들어, "중앙대학교"라는 키를 기준으로 등장 횟수를 모두 더함.
- **결과**: "중앙대학교" → 총 6번 등장
- 이 과정에서 Key는 변하지 않고, Value는 집계된 결과로 변환된다.

## 전체 프로세스 요약:

### Map 단계:
- **Key**: 문서 파일명이 특정 키워드("중앙대학교")로 변환.
- **Value**: 문서에서 해당 키워드의 등장 빈도로 변환.

### Group by Key 단계:
- **솔트(Salt)**: 키에 랜덤 값을 더해 동일한 키가 한 리듀서에 집중되지 않도록 분산.
- **셔플(Shuffle)**: 동일한 키를 가진 데이터를 할당된 리듀서로 전달하여 그룹화.

### Reduce 단계:
- 그룹화된 데이터를 집계하여 최종적으로 Key에 해당하는 등장 빈도를 결과로 도출.

![alt text](image-7.png)



### 예시의 단계별 설명:

### 1. **Input**:

- **입력 데이터**: 큰 문서, 예를 들어 `"Deer Bear River Car Car River Deer Car Bear"` 와 같은 텍스트임.
- 문서는 여러 부분으로 나뉘며, 각각을 분리하여 처리함.

### 2. **Splitting**:

- 문서를 여러 조각으로 나누는 과정임.
- 여기서는 각 줄 또는 구문을 분리하여 각 조각을 개별적으로 처리할 수 있도록 준비함.
- 예:
    - 첫 번째 조각: `"Deer Bear River"`
    - 두 번째 조각: `"Car Car River"`
    - 세 번째 조각: `"Deer Car Bear"`

### 3. **Mapping**:

- 각 조각에서 단어를 추출하고, **Key-Value 쌍**으로 변환함.
- 각 단어(Key)마다 1이라는 값(Value)을 할당하여 등장 빈도를 나타냄.
- 예:
    - `"Deer Bear River"` -> `("Deer", 1), ("Bear", 1), ("River", 1)`
    - `"Car Car River"` -> `("Car", 1), ("Car", 1), ("River", 1)`
    - `"Deer Car Bear"` -> `("Deer", 1), ("Car", 1), ("Bear", 1)`

### 4. **Shuffling** (Group by Key):

- 같은 Key(단어)를 가진 모든 쌍들을 그룹화함.
- 동일한 단어들을 그룹으로 모아, 해당 단어가 여러 줄에서 어떻게 등장하는지 한데 모음.
- 예:
    - `"Bear"` -> `("Bear", 1), ("Bear", 1)`
    - `"Car"` -> `("Car", 1), ("Car", 1), ("Car", 1)`
    - `"Deer"` -> `("Deer", 1), ("Deer", 1)`
    - `"River"` -> `("River", 1), ("River", 1)`

### 5. **Reducing**:

- 같은 Key를 가진 모든 Value를 합산하여 최종 빈도를 계산함.
- 이 단계에서는 그룹화된 Value를 집계하거나 요약함.
- 예:
    - `"Bear"` -> 2 (총 2번 등장)
    - `"Car"` -> 3 (총 3번 등장)
    - `"Deer"` -> 2 (총 2번 등장)
    - `"River"` -> 2 (총 2번 등장)

### 6. **Final Result**:

- 최종적으로 각 단어(Key)와 그 빈도(Value)를 가진 리스트로 결과가 출력됨.
- 예:
    - `("Bear", 2), ("Car", 3), ("Deer", 2), ("River", 2)`

### 전체 프로세스 요약:

1. **Map** 단계에서는 입력 데이터를 읽고, 각 단어를 Key로, 1을 Value로 하는 Key-Value 쌍을 생성함.
2. **Shuffle** 단계에서는 동일한 Key를 가진 쌍들을 그룹화함.
3. **Reduce** 단계에서는 그룹화된 Key-Value 쌍을 집계하여 최종적으로 각 Key(단어)마다 빈도를 계산함.

이 과정은 병렬로 수행될 수 있어 대용량 데이터를 빠르게 처리할 수 있음. Map 단계와 Reduce 단계는 프로그래머가 정의하는 함수로 구현됨.

### 의사코드

```python
def map(key, value):
    # key: document name; value: text of the document
    for w in value:
        emit(w, 1)

def reduce(key, values):
    # key: a word; value: an iterator over counts
    result = 0
    for v in values:
        result += v
    emit(key, result)
```
### MapReduce 환경이 처리하는 작업:
- Partitioning: 입력 데이터를 분할함
- Scheduling: 프로그램 실행을 여러 기계에 걸쳐 스케줄링함
- Group by Key 수행: 동일한 키를 가진 데이터를 그룹화함
- Handling machine failures: 기계 오류를 처리함
- Managing inter-machine communication: 기계 간 통신을 관리함

##### MapReduce 환경은 이러한 복잡한 작업들을 자동으로 처리해주기 때문에 개발자는 Map과 Reduce 함수에 집중할 수 있다.

# 워크플로우

![alt text](image-8.png)

1️⃣ **입력과 최종 출력**은 분산 파일 시스템(HDFS)에 저장됨

➡️ 스케줄러는 입력 데이터를 처리할 때 **맵 작업**을 데이터가 저장된 **물리적** 위치와 가까운 곳에서 실행하려고 함.

2️⃣ **중간 결과값**은 맵과 리듀스서의 **로컬 파일 시스템**에 저장됨.

3️⃣ Map task 순서:

- 입력 데이터가 **Split**으로 나눠짐
- **Record Reader**가 데이터를 읽고 **Map** 작업이 시작됨
- **Combine** 과정이 있을 수 있음(맵 출력의 중복 제거 및 축소)
- 중간 결과가 **로컬 스토리지**에 저장됨

4️⃣ Reduce task **순서**:

- `리듀서`가 맵 작업의 중간 결과를 로컬 파일 시스템에서 **복사**해옴, 이 과정에서 Map Task들이 저장한 로컬 데이터를 복사
- **정렬(Sort)** 과정으로 데이터를 키에 따라 정리
- **Reduce** 작업으로 최종 결과를 도출함

5️⃣ 리듀스 작업이 끝난 결과는 다시 **HDFS에 저장**됨.

#### 중요한 점:
맵 작업에서 생성된 중간 결과값은 로컬 파일 시스템에 저장되며, 이는 네임노드조차 그 위치를 알 수 없다. 중간 결과는 네트워크 전송 효율을 높이기 위해 로컬 파일 시스템에 남아 있고, 리듀서가 데이터를 복사해갈 때까지 보존됨.

## Master node <네임노드>의 역할

### **Coordination: Master (마스터 노드의 조정 역할)**

**마스터 노드는 작업의 전반적인 조정 역할을 담당:**

1. **작업 상태 관리 (Task Status)**
    - 작업 상태: (대기, 진행 중, 완료)
    - 작업의 진행 상황을 추적하고 관리

2. **대기 작업 처리 (Idle Tasks)**
    - 작업자(Worker)가 사용 가능해지면 대기 중인 작업이 할당됨.
    - 작업자들의 리소스를 최대한 활용할 수 있도록 효율적인 작업 스케줄링이 이루어짐.

3. **맵 작업 완료 시 (When a Map Task Completes)**
    - 각 맵 작업이 완료되면, 해당 **매퍼**가 **마스터 노드에게** 완료된 맵 작업의 중간 결과 파일(R 파일)의 위치와 크기를 전송.
    - **마스터 노드**는 이 정보를 취합한 뒤, 각 **리듀서에게** 필요한 중간 데이터를 **전달**함.

4. **작업자 상태 모니터링 (Heartbeat)**
    - 마스터 노드는 작업자들에게 주기적으로 핑을 보내 하트비트(heartbeat)를 확인하여, 작업자가 정상적으로 동작하고 있는지 또는 결함이 발생했는지 감지.
    - 이를 통해 작업자가 실패한 경우 빠르게 대처할 수 있음.

### **분산 파일 시스템에서 장애 처리 (Dealing with Failures)**

### **장애 내성을 위한 재실행 메커니즘 (Re-execution for Fault-Tolerance)**

- 분산 파일 시스템에서 **fault-tolerance**는 시스템의 중요한 특징으로, 시스템의 안정성과 연속성을 유지하기 위한 핵심 요소.

### **Worker Failure:**

1. **마스터 노드에서의 실패 감지**:
    - **Heartbeat**: **Master 노드**는 **worker 노드**의 주기적인 **heartbeat 신호**를 통해 **worker failure**를 감지하는데, 이 신호가 일정 시간 동안 감지되지 않으면 **worker failure**로 간주함.

2. **재실행의 필요성**:
    - **Map 작업 재실행**: 장애가 발생하면, **완료된 map 작업**과 **진행 중인 map 작업** 모두 재실행함.
        - 특히 완료된 map 작업도 재실행되는 이유는, 분산 파일 시스템의 특성상, 작업이 완료되었더라도 장애가 발생한 노드에서 데이터 무결성을 보장할 수 없기 때문임. 즉, 데이터 손실이나 불일치 문제가 발생할 수 있으므로 **모든 map 작업**이 다시 실행됨.
    - **Reduce 작업 재실행**: 반면, **reduce 작업**의 경우, **진행 중인 작업**만 재실행함. 이는 reduce 작업이 map 작업 이후에 데이터를 종합하는 작업이므로, 이미 완료된 작업은 데이터를 충분히 보존하고 있으며 장애의 영향을 받지 않기 때문임.

### **Master Failure:**

1. **초기 구현 문제**:
    - 분산 시스템의 초창기 구현에서는 **마스터 노드의 장애**가 발생할 경우 이를 복구할 메커니즘이 부재했음. **MapReduce 작업**은 즉시 중단되며, **클라이언트에게 장애 알림**이 전송됨. 이는 시스템의 전체 작업 흐름에 심각한 영향을 미치는 큰 문제였음.

2. **해결책**:
    - **상태 저장**: 마스터 노드의 장애 발생 시 시스템의 중단을 최소화하기 위해, **DFS(Distributed File System)**의 내부 구조 상태를 정기적으로 체크포인트로 저장하는 방법이 도입됨. 이를 통해 마스터 노드가 장애에서 복구될 때, 작업을 중단한 지점에서부터 다시 시작할 수 있음.
    - **복제 기술 활용**: 마스터 노드의 역할을 분산하고 복제하여 장애 시에도 다른 노드가 동일한 역할을 수행할 수 있도록 하는 **replication** 기술이 사용됨. 이로서 **single point of failure** 문제를 방지할 수 있음.



### 파티션을 제어하려는 이유:
- Map 작업의 입력은 입력 파일의 연속적인 조각(splits)으로 나뉜다.
- Reduce 작업에서는 동일한 중간 키(Intermediate Key)를 가진 레코드들이 같은 워커(Worker)에게 전달되어야 한다. `즉, 동일한 키는 동일한 리듀서에서 처리되어야 한다.`
- 이 때, `파티셔너`는 키가 어느 리듀서로 가는지를 결정한다.

### 기본 파티셔너 함수:
- 시스템은 기본적으로 해시 함수를 사용하여 키를 분할한다. 기본 파티셔닝 함수는:
  \[
  \text{hash(key)} \mod R
  \]
  여기서 R은 리듀서의 수이다. 즉, 키의 해시 값을 R로 나눈 나머지 값을 통해 해당 키가 어느 리듀서로 전달될지 결정된다.
- 이는 키 분배가 고르게 이루어지도록 하는 간단한 방식이다.

### 해시 함수를 덮어쓰는 경우:
- 때로는 기본 해시 함수를 덮어쓰는 것이 유용할 수 있다. 예를 들어:
  - \(\text{hash(hostname(URL))} \mod R\): 특정 호스트에서 나온 URL들이 동일한 출력 파일에 모이도록 하고 싶을 때 유용하다.
  - 이 예시는 특정한 기준으로 데이터를 그룹화해야 할 때 커스텀 파티셔너가 필요할 수 있다는 점을 보여준다.

## 요약:

- Partitioner는 키가 어느 리듀서로 분배될지를 결정하는 메커니즘이다.
- 시스템은 기본적으로 해시 함수를 사용해 데이터를 분배한다.
- 상황에 따라 해시 함수를 덮어쓰고 특정 키가 특정 리듀서로 가도록 설정할 수 있다.

이러한 방식으로 파티셔너는 분산 데이터 처리에서 효율성을 극대화하는 중요한 역할을 수행한다.