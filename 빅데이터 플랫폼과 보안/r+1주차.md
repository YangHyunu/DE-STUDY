# 스트림을 위한 추가 알고리즘:

1. **데이터 스트림 필터링: 블룸 필터 (Bloom filters)**
   - 스트림에서 속성 x를 가진 요소 선택

2. **고유 요소 개수 세기: 플라조렛-마틴 (Flajolet-Martin)**
   - 스트림의 마지막 k 요소에서 고유한 요소의 개수

3. **모멘트 추정: AMS 방법**
   - 마지막 k 요소의 표준 편차 추정

4. **빈번한 항목 찾기: 지수적으로 감소하는 윈도우 (Exponentially decaying Windows)**
   - 현재 가장 인기 있는 요소 선택

# 1. Filtering Data Streams: Bloom Filters <멤버십 테스트>

## 목적:

- 데이터 스트림의 각 요소를 튜플이라고 가정하고, S라는 키값을 저장하는 리스트가 주어졌을 때, 새로운 데이터 스트림이 들어왔을 때 기존 리스트에 있는지 없는지 Yes or No를 알려주는 역할.
- 즉: 스트림에서 특정 데이터 요소의 **존재여부**를 확인하는 역할

#### 직관적인 Solution: Hash Table
- 문제점: Data stream의 수가 너무 커서 Hash Table에 사실상 데이터를 저장할 수 없음. 즉, 매핑 불가
#### Example:

1. **Email Spam Filtering**
    - 스팸 메일이 아닌 메일 주소를 10억 개 가지고 있다고 가정. 
    새로운 메일이 왔을 때, 메일 주소가 보유한 주소에 `속하지 않으면` 스팸 메일일 가능성이 있다고 `판단`.

2. **Publish-Subscribe Systems**
    - **다수의 메시지를 수집**:
        - 예를 들어 뉴스 기사와 같은 많은 메시지를 수집.
    - **캐시 시스템**: 웹서버에서 데이터 요청이 캐시에 있는지 `여부`를 확인할 때 사용.
         - 사용자가 자주 요청하는 데이터가 캐시에 있을 경우, 이를 빠르게 찾기 위해 블룸 필터를 사용하여 캐시에 데이터가 있는지 여부를 확인.
         - 데이터가 캐시에 있다고 블룸 필터가 말하면 실제로 있는지 확인하고, 없다면 데이터를 원본 저장소에서 불러옴.
    - **각 메시지가 구독자의 관심사와 일치하는지 확인**:
        - 시스템은 수집된 각 메시지가 구독자가 설정한 키워드와 일치하는지 여부를 확인하고, 일치할 경우 구독자에게 전달.


### First Cut Solution (1)
## 정리

### 해시 함수의 역할
- 해시 함수는 주어진 입력(예: URL)을 받아 고정된 범위의 숫자로 변환한다. 이 변환된 숫자가 바로 해시 값이다.
- 해시 값은 비트 배열 B에서 **특정 인덱스(위치)**를 나타낸다.

### 해시 값의 사용
- 해시 값을 통해 URL을 비트 배열 B의 특정 위치에 매핑할 수 있다.
- 해시 값이 나타내는 위치에 해당하는 비트는 1로 설정되어, 해당 URL이 그 위치에 매핑되었음을 기록한다.

### 과정
- URL이 해시 함수에 의해 해시 값으로 변환되면, 이 값이 비트 배열 B에서 해당하는 위치를 나타내게 된다.
- 해당 위치의 비트가 1로 설정되면, 이 URL은 비트 배열에 기록된 것이다.

### 실시간 스트림 처리
- 실시간으로 들어오는 URL을 다시 해시 함수로 변환하여, 그 해시 값이 비트 배열에서 1로 설정된 위치와 일치하는지 확인한다.
- 일치할 경우, 이 URL이 S 집합에 포함될 가능성이 있다고 판단한다.

### 테이블

| 단계                  | 설명                                                                                           |
|-----------------------|----------------------------------------------------------------------------------------------|
| 1. 키 집합 S 설정     | 필터링하려는 URL 집합 S를 정의한다. 예: {"https://example.com", "https://openai.com", "https://github.com"}. |
| 2. 비트 배열 B 생성   | n개의 비트로 이루어진 배열 B를 생성하고, 모든 비트를 0으로 초기화한다.                             |
| 3. 해시 함수 h 선택   | 도메인이 [0, n]인 해시 함수 h를 선택한다. 해시 함수는 **URL을 고정된 범위의 숫자(해시 값)**으로 변환한다.  |
| 4. S의 각 URL을 해시화 | S의 각 URL s를 해시 함수로 변환하여, 비트 배열 B의 특정 위치에 매핑한다. 예: "https://openai.com"의 해시 값이 4라면, B 배열의 4번째 비트를 1로 설정한다. |
| 5. 스트림의 각 URL 처리 | 스트림에서 들어오는 URL a를 해시 함수에 넣고, 해시 값이 B 배열의 1로 설정된 비트에 해당하는지 확인한다.       |
| 6. 출력 조건          | a의 해시 값이 B 배열에서 1로 설정된 비트에 해당하면, a를 출력한다.                                 |

### 예시 과정

1. **URL 집합 S 설정**:
   - S = {"https://example.com", "https://openai.com", "https://github.com"}

2. **비트 배열 B 생성**:
   - 비트 배열 B는 크기 n = 10으로 설정되고, 초기 상태는 모두 0이다.
   - 예: B = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

3. **해시 함수 h 선택**:
   - 예를 들어, 해시 함수 h는 URL을 입력받아 0에서 9 사이의 정수를 반환하는 함수로 정의된다.

4. **URL 해시화**:
   - "https://example.com" → h("https://example.com") = 2
     - B 배열의 2번째 비트를 1로 설정.
     - B = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
   - "https://openai.com" → h("https://openai.com") = 4
     - B 배열의 4번째 비트를 1로 설정.
     - B = [0, 0, 1, 0, 1, 0, 0, 0, 0, 0]
   - "https://github.com" → h("https://github.com") = 7
     - B 배열의 7번째 비트를 1로 설정.
     - B = [0, 0, 1, 0, 1, 0, 0, 1, 0, 0]

5. **실시간 URL 처리**:
   - 스트림에서 "https://openai.com"이 들어오면, 해시 함수 h에 넣어 해시 값을 구한다.
     - h("https://openai.com") = 4
     - B 배열의 4번째 비트가 1로 설정되어 있으므로, "https://openai.com"은 S에 포함될 가능성이 있다. 이를 출력한다.

6. **출력 조건**:
   - 만약 해시 값이 1로 설정된 비트와 일치한다면, 해당 URL을 출력한다. 예를 들어, "https://openai.com"은 출력된다.

![alt text](image-18.png)

- **Item**: 필터에 넣고자 하는 항목
- **Hash Function h**: 항목을 해시 함수에 입력하여 비트 배열 B의 특정 위치를 결정. 여러 해시 함수가 사용될 수 있다.
- **Bit Array B**: 0과 1로 이루어진 배열로, 각 비트는 집합에서 항목이 포함되었는지를 확인함.

  - **비트가 0인 경우**: 항목은 집합에 확실히 포함되지 않음을 의미.
  - **비트가 1인 경우**: 항목이 집합에 있을 가능성이 있지만, 반드시 포함되어 있다는 의미는 아니다.
  - (거짓 긍정 발생 가능성): 이유는 `Hash Collision`이 발생할 수 있기 때문

#### 워크 플로우:
- 항목이 해시 함수에 의해 특정 위치에 매핑되고, 해당 비트가 0이면 해당 항목이 집합에 없다고 확신할 수 있다. 즉, false negative가 발생하지 않는다.
- 항목이 해시된 위치의 비트가 1이면, 집합에 있을 수도 있음을 의미한다. 그러나, 이 경우 항목이 실제로 포함되지 않았지만 포함되었다고 판단할 가능성이 존재한다. 이는 Hash Collision 가능성이 있기 때문이다. 따라서 false positive가 발생할 수 있다.

### 정리
Bloom Filter는 빠르고 효율적으로 집합에 항목이 있는지 확인할 수 있지만, 거짓 긍정(항목이 없는데 있다고 판단)은 발생할 수 있다. 반면, 거짓 부정(항목이 있는데 없다고 판단)은 발생하지 않는다.

#### Filter의 Output이 1이어도 확실히 새로운 값이 기존에 있었다고 할 수 없다.

- 이유: **Hash Collision**
    - Hash collision은 서로 다른 입력 값이 동일한 해시 값을 생성하는 현상으로, 이는 해시 함수의 특성상 무한한 입력 도메인을 유한한 해시 값 범위로 매핑하기 때문에 발생한다. 블룸 필터에서 이러한 충돌은 false positive 결과를 초래할 수 있다.
    - 즉, Hash 함수가 1을 리턴했더라도 해당 스트림의 요소 a가 집합 S에 있다고 할 수 없다.

**Hash Collision의 해결방안**

- 블룸 필터에서 false positive(에러)를 가정하면서 false negative의 상황은 가정하지 않음.

### First Cut Solution (3)
1. **집합 S의 크기**:
   - 집합 S에는 10억 개의 이메일 주소가 있다고 가정하자, 이 Bloom Filter는 들어오는 이메일 데이터가 10억 개의 이메일 주소가 포함되어 있는지 아닌지를 빠르게 검사하려고 한다.

2. **비트 배열 B의 크기**:
   - 비트 배열 B는 1GB 크기로 설정되어 있고, 이는 80억 비트(8 billion bits)로 이루어져 있다. 이메일 주소를 해시 함수로 변환해 이 비트 배열의 특정 위치를 1로 설정하는 방식으로 작동한다.

3. **거짓 부정 없음 (No False Negatives)**:
   - **거짓 부정(false negative)**: 실제로는 집합에 포함되어 있는데, Bloom Filter가 포함되어 있지 않다고 잘못 판단하는 상황.
   - Bloom Filter에서는 거짓 부정이 발생하지 않는다. 즉, 이메일 주소가 집합 S에 포함되어 있으면, Bloom Filter는 항상 포함되었다고 인식한다.
   - 이는 여러 해시 함수를 사용하여 비트 배열의 여러 위치를 설정하기 때문에 가능하다.

4. **거짓 긍정 발생 (False Positives)**:
   - **거짓 긍정(false positive)**: 실제로는 집합에 포함되어 있지 않는데, Bloom Filter가 포함되었다고 잘못 판단하는 상황.
   - 비트 배열의 약 1/8이 1로 설정될 것이며, 이로 인해 집합에 포함되지 않은 이메일 주소 중 약 1/8이 거짓으로 포함된 것처럼 보일 수 있다.
   - 이는 여러 해시 함수로 인해 비트 배열의 특정 위치가 겹칠 수 있기 때문이다. 즉, 다른 이메일 주소가 비슷한 해시 값을 가져와 비트 배열의 동일한 위치를 설정하면서 발생하는 해시 충돌로 인해 거짓 긍정이 발생할 수 있다.

### 요약:
- **거짓 부정 없음**: 실제로 집합 S에 포함된 이메일 주소는 Bloom Filter에서 항상 포함되었다고 올바르게 판단한다.
- **거짓 긍정 발생**: 집합 S에 포함되지 않은 이메일 주소도 해시 충돌로 인해 포함된 것처럼 보일 수 있으며, 그 확률은 대략 1/8 정도이다.

![alt text](image-19.png)


##  Q: n개의 다트와 m개의 타겟이 있을 때, 타겟이 적어도 하나 이상의 다트에 맞을 확률은?  
=> 우리의 케이스에서 n개를 stream input으로, m을 해시된 값의 집합 B의 크기로 가정한다면:

False Positive 확률로 해석할 수 있다:

- 각 타겟 X가 적어도 하나 이상의 다트에 맞을 확률을 False Positive 확률로 해석할 수 있다.
- False Positive는 존재하지 않는 데이터가 비트 배열에 설정된 1과 충돌하여 마치 존재하는 것처럼 잘못 인식되는 경우를 의미한다.

## 1. 1/m의 의미

- 1/m은 하나의 데이터(다트)가 m개의 버킷(타겟) 중 `특정`한 하나의 버킷에 매핑될 확률을 나타낸다.
- 예를 들어, 100개의 버킷이 있을 때, 하나의 데이터를 해시하여 특정한 하나의 버킷에 매핑될 확률은 1/100이다.
- 즉, 1/m은 각각의 데이터가 특정한 버킷을 맞출 확률을 의미한다.

## 2. 1 - 1/m의 의미

- 1 - 1/m은 특정한 하나의 버킷에 데이터가 매핑되지 않을 확률이다.
- 예를 들어, 100개의 버킷이 있을 때, 하나의 데이터가 `특정`한 하나의 버킷에 매핑되지 않을 확률은 1 - 1/100 = 99/100이다.
- 즉, 1 - 1/m은 데이터가 `특정` 버킷에 매핑되지 않을 확률을 나타낸다.

## 3. n개의 데이터가 모두 특정한 하나의 버킷에 매핑되지 않을 확률

- n개의 데이터가 모두 `특정`한 하나의 버킷에 매핑되지 않을 확률을 계산하려면, **(1 - 1/m)**을 n번 곱해야 한다.
- 이를 수식으로 나타내면:
  \[
  (1 - \frac{1}{m})^n
  \]
- 이 수식은 n개의 데이터 중 어느 하나도 `특정`한 버킷에 매핑되지 않을 확률을 의미한다.
- 즉, 모든 데이터가 특정 버킷을 피해 갈 확률이다.

## 4. n개의 데이터 중 적어도 하나가 `특정`한 하나의 버킷에 매핑될 확률

- n개의 데이터 중 적어도 하나가 `특정`한 하나의 버킷에 매핑될 확률을 구하려면, 전체 확률 1에서 모든 데이터가 그 버킷에 매핑되지 않을 확률을 빼면 된다:
  \[
  1 - (1 - \frac{1}{m})^n
  \]
- 이 식은 n개의 데이터 중 적어도 하나 이상의 데이터가 `특정`한 버킷에 매핑될 확률을 나타낸다.
- 여기서 **"적어도 하나"**는 최소한 1개 이상의 데이터가 그 버킷에 매핑되어 버킷이 1로 설정될 가능성을 의미한다.

## 5. 지수 함수로의 근사화

- \((1 - \frac{1}{m})^n\)은 수학적으로 지수 함수로 근사할 수 있다. 특히, n과 m이 매우 큰 경우, \((1 - \frac{1}{m})^n\)은 **e^{-n/m}**으로 근사된다.
- 따라서, False Positive 확률을 더 간단하게 계산할 수 있다:
  \[
  1 - e^{-\frac{n}{m}}
  \]

## 6. 결론

- False Positive 확률은 n개의 데이터 중 적어도 하나가 `특정`한 버킷에 매핑되어 그 버킷이 1로 설정될 확률로 계산된다:
  \[
  P(\text{False Positive}) = 1 - e^{-\frac{n}{m}}
  \]

### 예시

- 10억 개의 다트와 80억 개의 타겟이 있을 때 (즉, \( n = 10^9 \), \( m = 8 \times 10^9 \)):
  \[
  P = 1 - e^{-\frac{10^9}{8 \times 10^9}} = 1 - e^{-\frac{1}{8}} \approx 0.1175
  \]
- 비트 배열에서 1로 설정된 비트의 비율은 0.1175로 계산된다. 즉, 약 11.75%의 확률로 False Positive가 발생할 수 있음을 의미한다.

## 블룸 필터 (Bloom Filter)에서 False Positive 확률 줄이기

- K개의 독립적인 해시 함수를 사용하면 False Positive 확률을 줄일 수 있다. 각 해시 함수가 서로 다른 위치에 데이터를 매핑함으로써, 더 정확한 필터링이 가능하다.

### 블룸 필터 구성

- **집합 S 크기**: \( |S| = n \)
  - 예: S는 n개의 요소(예: 이메일 주소)를 포함하는 집합이다.
- **비트 배열 B 크기**: \( |B| = m \)
  - 예: B는 m개의 비트로 구성된 배열로, 각 비트는 특정 요소가 집합에 포함될 가능성을 나타낸다.
- **해시 함수**:
  - k개의 독립적인 해시 함수 \( h_1, h_2, \ldots, h_k \)를 사용한다.
  - 각 해시 함수는 집합의 요소 s를 비트 배열 B의 서로 다른 위치로 매핑한다.
- **비트 배열 초기화**:
  - 비트 배열 B의 모든 요소를 0으로 설정한다.
  - 집합 S의 각 요소 \( s \in S \)에 대해 각 해시 함수 \( h_i \)로 해시를 수행하고, B의 해당 위치에 1을 설정한다.

### 실행 시간 (Runtime)

- x가 도착했을 때:
  - 각 해시 함수로 x를 해시하여, 각 해시 값에 대응되는 비트가 1로 설정되어 있는지 확인한다.
  - 모든 해시 결과가 1이면, x는 집합 S에 포함될 가능성이 있다.
  - 하나라도 0이면, x는 집합 S에 포함되지 않는다.

### False Positive 줄이기

- K개의 해시 함수를 사용함으로써, 여러 위치에서 데이터를 확인할 수 있다. 하나의 해시 함수만 사용했을 때보다 False Positive 발생 확률을 줄일 수 있다.
- 다양한 위치에서 비트를 확인함으로써, 단일 해시 충돌로 인해 발생할 수 있는 오류를 줄인다.

### 블룸 필터 적용 시 False Positive 확률

- K개의 해시 함수를 사용한 False Positive 확률은 다음과 같이 계산된다:
  \[
  P(\text{False Positive}) = (1 - e^{-\frac{kn}{m}})^k
  \]
  - 여기서 k는 해시 함수의 개수이다.
  - n은 데이터의 개수, m은 비트 배열의 크기이다.
  - K개의 해시 함수를 사용하면 여러 위치에서 데이터를 확인하므로, False Positive 확률을 줄일 수 있다.

## 최종 정리

- **1/m**: 하나의 데이터가 특정한 하나의 버킷에 매핑될 확률.
- **1 - 1/m**: 하나의 데이터가 특정한 하나의 버킷에 매핑되지 않을 확률.
- \((1 - \frac{1}{m})^n\): n개의 데이터가 모두 특정한 하나의 버킷에 매핑되지 않을 확률.
- **1 - (1 - 1/m)^n**: n개의 데이터 중 적어도 하나가 특정한 하나의 버킷에 매핑될 확률 (이것이 False Positive 확률이다).
- **근사화**: 이를 지수 함수로 근사화하면, False Positive 확률은 **1 - e^{-n/m}**로 계산된다.

### False Positive 확률 계산에 대한 설명

- False Positive는 실제로는 포함되지 않은 데이터가 비트 배열에서 1로 설정된 비트와 충돌하여 존재하는 것처럼 오인되는 상황이다. 이를 계산하기 위해, n개의 데이터가 m개의 비트 배열에 매핑될 때, 적어도 하나의 데이터가 특정 비트에 매핑될 확률을 구한다.
- 이 과정에서 **1 - (1 - 1/m)^n** 또는 근사적으로 **1 - e^{-n/m}**를 사용하여 False Positive 확률을 계산하게 된다.

##  블룸필터 정리
블룸 필터의 장점:

1. **거짓 부정(false negatives)이 없고 메모리를 적게 사용**:
   - 복잡한 검사를 수행하기 전에 빠른 사전 처리 단계로 사용하기에 적합하다.
   - 블룸 필터를 사용하여 더 비용이 많이 드는 검사를 수행하기 전에 효율적으로 필터링할 수 있다.

2. **하드웨어 구현에 적합**:
   - 해시 함수 계산은 병렬 처리가 가능하므로, 하드웨어에서 쉽게 구현할 수 있다.
   - 병렬성을 활용하면 여러 해시 함수를 동시에 계산하여 효율적으로 처리할 수 있다.

3. **하나의 큰 비트 배열 B vs 여러 개의 작은 비트 배열 B**:
   - 큰 비트 배열 하나를 사용할지, 여러 개의 작은 비트 배열을 사용할지에 대한 질문이다.
   - 계산식은 동일하다:
     - 하나의 큰 비트 배열을 사용할 경우: \( (1 - e^{-kn/m})^k \)
     - 여러 개의 작은 비트 배열을 사용할 경우: \( (1 - e^{-n/(m/k)})^k \)
   - 하지만 하나의 큰 비트 배열을 유지하는 것이 더 간단하다.

   - 즉, 큰 비트 배열 하나를 사용하는 것과 여러 개의 작은 비트 배열을 사용하는 것은 확률적으로 동일하지만, 큰 배열 하나를 사용하는 것이 더 단순한 방법이다.

블룸 필터는 false negative가 없고 메모리를 절약하며, 빠른 사전 처리에 적합한 도구이다. 

# 2) Counting Distinct Elements

## 문제 설명:

- 데이터 스트림: 요소가 연속적으로 스트리밍되고 있으며, 각 요소는 크기가 N인 집합에서 선택됨.
목표: 지금까지 본 **고유한 요소의 개수(서로 다른 요소)**를 유지하고 추적하는 것.

## 직관적인 접근법:

- 직접적으로 모든 요소를 저장하는 방법. 
    - 이 방법은:
    지금까지 본 모든 요소를 해시 테이블에 저장하고,
    새로운 요소가 들어올 때마다 해시 테이블을 확인해서 `중복이 아니면` 추가하는 방식으로 , 결국 해시 테이블에 저장된 요소들의 수가 곧 고유한 요소들의 수가 되게 됨.

##### 문제점: 
      1. 공간 복잡도: 들어온 요소들을 모두 해시 테이블에 저장하므로 엄청난 메모리 공간이 필요함, 심지어 데이터 스트림이 무한한 경우 이는 불가능함
      2. 시간 복잡도: 매번 해시 테이브을 업데이트 하고 중복을 체크하는데 시간을 소모함 즉, 요소가 많아질 수록 기하급수적으로 해시 테이블을 검색하는 데 시간이 오래 걸릴 수 있음.

### 응용 사례:

1. **웹 페이지 크롤링 중 발견된 고유한 단어 수 추적**:
   - **문제**: 특정 웹 사이트에서 크롤링한 페이지들에서 고유한 단어가 몇 개나 발견되는지를 추적하는 것.
   - **응용**: 너무 적거나 너무 많은 고유 단어가 발견되면, 이 페이지가 `스팸`이나 인위적으로 생성된 페이지일 가능성이 있다는 의심을 할 수 있다.

2. **각 고객이 일주일 동안 요청한 고유 웹 페이지 수**:
   - **문제**: 각 고객이 일주일 동안 얼마나 많은 고유한 웹 페이지를 요청했는지를 추적한다.
   - **응용**: 고객의 웹 사이트 `사용 패턴`을 파악하거나 `이상 징후`를 `탐지`할 수 있다.

3. **지난 주에 판매된 고유 제품 수 추적**:
   - **문제**: 지난 일주일 동안 판매된 고유 제품이 몇 개나 되는지 확인한다.
   - **응용**: 판매 기록에서 고유한 제품을 확인하고, 재고 관리나 판매 `트렌드 분석`에 사용할 수 있다.

#### 현실적인 문제:
Q: 지금까지 본 요소들의 집합을 유지할 공간이 없다면 어떻게 할까?
A: 약간의 오차를 허용하되, 그 오차가 크게 발생할 확률을 제한한다.

## Flajolet-Martin 알고리즘 <근사 알고리즘>
1. 해시 함수 h 선택:

   - **해시 함수 h**는 균일한 분포(uniform distribution)를 가진다고 가정한다. 즉, h는 주어진 N개의 요소를 임의의 값에 균일하게 매핑한다.
   - 이 해시 함수는 각 요소를 최소한 \(\log_2 N\) 비트로 표현한다. \(\log_2 N\) 비트는 N개의 요소를 표현하기 위한 최소 비트 수이다. 예를 들어, 16개의 요소를 표현하려면 \(\log_2 16 = 4\) 비트가 필요하다.

2. 각 스트림 요소에 대해 후행 0의 수 계산:

   - **스트림의 각 요소 a**에 대해, 해시 함수 \( h(a) \)가 반환하는 값을 이진수로 변환한 뒤, 오른쪽 끝에 있는 연속된 0의 개수를 계산한다. 이를 후행 0의 수라고 한다.
   - 예시:
     - \( h(a) = 12 \)라고 가정해 보자. 12는 이진수로 1100이다.
     - 이진수의 오른쪽 끝부터 시작해 0이 몇 개 연속으로 나오는지 세면, 2개의 0이 있다.
     - 따라서 \( r(a) = 2 \)이다. 여기서 **r(a)**는 a 요소의 후행 0의 개수를 나타낸다.

3. 최대 후행 0의 수 R 기록:

   - 데이터를 처리하는 동안, 각 요소 a에 대해 계산된 후행 0의 수 \( r(a) \) 중에서 **최댓값 R**을 기록한다.
   - 즉, 모든 요소에 대해 가장 긴 후행 0의 개수를 저장한다.
   - R은 지금까지 본 요소 중에서 최대 후행 0의 수를 의미한다.
     - 예를 들어, \( r(a_1) = 2 \), \( r(a_2) = 3 \)이면, \( R = 3 \)이 된다.

4. 추정된 고유한 요소의 수:

   - 고유한 요소의 수를 추정하기 위해, **최댓값 R**을 사용한다.
   - **고유한 요소의 수를 \( 2^R \)**로 추정한다.
     - 예를 들어, \( R = 3 \)이면, 고유한 요소의 수는 \( 2^3 = 8 \)로 추정된다.
     - 이 값은 HyperLogLog 알고리즘에서 고유한 요소 수를 근사하는 간단한 방식 중 하나이다.

#### 이 방법이 작동하는 이유:

- 후행 0의 개수가 많을수록, 이진수로 표현된 숫자는 더 큰 범위에서 임의로 분포되었을 가능성이 크다.
- 따라서, 후행 0의 수가 많다는 것은 고유한 값들이 많이 들어왔을 가능성이 크다는 뜻이다.
- 이 방법은 매우 적은 메모리(공간)를 사용하면서도 고유한 요소의 수를 근사적으로 정확하게 추정할 수 있다.

**요약**:

- **해시 함수 h**로 각 스트림 요소 a를 해싱한다.
- 해시 결과의 이진 표현에서 오른쪽 끝에 있는 0의 개수를 센다. 이를 \( r(a) \)라고 한다.
- 지금까지 본 요소들 중에서 가장 큰 후행 0의 수 R을 기록한다.
- **고유한 요소의 개수를 \( 2^R \)**로 추정한다.

이 방식은 매우 큰 데이터 스트림에서 메모리 사용량을 최소화하면서도, 고유한 요소 수를 정확히 추정할 수 있다는 장점이 있다.

#### Heuristic intuition why Flajolet-Martin works:
1. **해시 함수 h(a)의 동작 원리**:
   - 해시 함수 h(a)는 동일한 확률로 N개의 값 중 하나로 요소 a를 해싱한다.
   - 이를 통해 각 요소는 동등한 확률로 특정 해시 값에 매핑된다.

2. **이진수 표현의 후행 0 개수**:
   - 해시 값 h(a)는 \(\log_2 N\) 비트의 이진수로 표현될 수 있다. N은 가능한 해시 값의 개수이다.
   - 각 해시 값은 확률적으로 후행 0의 개수가 결정된다.
   - **후행 0의 개수 r**가 r인 경우의 확률은 \(2^{-r}\)이다.
     - 만약 후행 0이 1개인 값을 찾고 싶다면, 이 확률은 \(2^{-1} = \frac{1}{2}\) 즉, 해시 값의 마지막 비트가 0일 확률은 50%
     - 만약 후행 0이 2개인 값을 찾고 싶다면, 그 확률은 \(2^{-2} = \frac{1}{4}\) 즉, 해시 값의 마지막 두 비트가 모두 0일 확률은 25%
3. **후행 0이 가장 긴 경우를 관찰**:
   - 예를 들어, 후행 0이 2개인 가장 긴 해시 값을 관찰했다고 가정한다 (r=2). 즉, 해시 값이 끝에 100과 같은 형태이다.
   - 이 경우, **우리가 본 서로 다른 요소의 수는 대략 \(2^r = 2^2 = 4\)**로 추정된다.
   - 후행 0의 개수가 많을수록 고유한 요소의 수가 더 많다는 것을 의미하며, 이는 고유한 요소의 수를 추정하는 근거가 된다.

4. **추정의 논리**:
   - **후행 0의 개수가 r**인 요소를 발견하려면, 약 **\(2^r\)**개의 고유한 요소를 봐야 한다는 의미이다.
   - 예를 들어, 후행 0의 개수가 r=2이면, 우리가 본 고유한 요소의 수는 약 4개라고 추정할 수 있다.
   - 이 원리를 이용해 **최대 후행 0의 개수 R**을 기록하고, **고유한 요소의 수를 \(2^R\)**로 추정하는 방식이 Flajolet-Martin 알고리즘의 핵심이다.

### 요약:
- 해시 함수는 요소를 동등한 확률로 해싱하며, 해시 값의 이진 표현에서 후행 0의 개수를 통해 고유한 요소의 수를 추정한다.
- 후행 0의 개수가 많을수록 고유한 요소의 수가 많다는 의미가 된다.
- 약 \(2^r\) 항목을 처리해야 후행 0이 r개인 요소를 발견할 수 있으며, 이 원리를 통해 고유한 요소의 수를 추정한다.

![alt text](image-22.png)
- R은 새로운 해시 값 \( h(a_i) \)가 들어올 때마다 최대 후행 0의 길이를 갱신한다.
- 그 결과, R은 어느 시점에서 6에 도달할 수 있다.
    - 만약 최종적으로 R = 6이 라면,
    - 추정된 고유 요소의 수 = \( 2^R = 2^6 = 64 \approx 100 \)

### Flajolet-Martin 알고리즘이 작동하는 이유 <수학적>

![alt text](image-23.png)

1. **후행 0의 개수 r을 찾을 확률**:
   - 주어진 해시 값 \( h(a) \)가 적어도 r개의 0으로 끝날 확률은 \( 2^{-r} \)이다.
   - m개의 고유 요소 중에서 r개의 0으로 끝나는 해시 값을 찾지 못할 확률은 \( (1 - 2^{-r})^m \)이다.

2. **확률 계산**:
   - \( m \ll 2^r \implies (1 - 2^{-r})^m \approx 1 \quad \text{as} \quad \frac{m}{2^r} \to 0 \)
     - 즉, r개의 0을 찾지 못할 확률이 1에 가까워지고, r개의 0을 찾을 확률은 0에 가까워진다.
   - \( m \gg 2^r \implies (1 - 2^{-r})^m \approx 0 \quad \text{as} \quad \frac{m}{2^r} \to \infty \)
     - 즉, r개의 0을 찾지 못할 확률이 0에 가까워지고, r개의 0을 찾을 확률은 1에 가까워진다.

3. **결론**:
   - \( m \ll 2^r \)이면, r개의 0을 찾을 확률은 0에 가까워진다.
   - \( m \gg 2^r \)이면, r개의 0을 찾을 확률은 1에 가까워진다.
   - 따라서, \( 2^R \)은 거의 항상 m에 가까운 값을 가진다.

### 요약

- Flajolet-Martin 알고리즘은 해시 값의 후행 0 개수를 통해 고유한 요소의 수를 추정한다.
- \( m \ll 2^r \)이면, r개의 0을 찾을 확률은 0에 가까워지고, \( m \gg 2^r \)이면, r개의 0을 찾을 확률은 1에 가까워진다.
- 따라서, \( 2^R \)은 거의 항상 m에 가까운 값을 가진다.

# 3) Computing Monments  AMS 방법 (Alon-Matias-Szegedy)

### 적률 계산
- 스트림의 요소가 N개의 값으로 구성된 집합 A에서 선택된다고 가정.
- \( m_i \)는 값 \( i \)가 스트림에서 나타난 횟수로 정의됨.
- k번째 모멘트는 다음과 같이 계산됨:
  \[
  \sum_{i \in A} (m_i^k)
  \]

### 예시:

- **이상 탐지(Anomaly Detection)**: 데이터 센터의 서버에서 비정상적인 트래픽 패턴을 감지하는 데 사용된다.
  - AMS 방법을 사용하여 서버 요청 로그에서 불균형한 `요청 패턴`을 찾아낼 수 있다. 특정 사용자가 다른 사용자에 비해 지나치게 많은 요청을 보내는 경우를 감지할 수 있다.

- **웹 로그 분석**: 사용자가 특정 페이지를 방문한 횟수의 `분포`를 분석하여, 가장 많이 방문한 페이지와 방문 횟수의 차이를 추정할 수 있다.
  - 사용자가 주로 어떤 페이지를 많이 방문하고 그 방문 수의 `편차`가 큰지 확인하여 인기 페이지와 비인기 페이지의 비율을 추정할 수 있다.

- **네트워크 트래픽 분석**: 네트워크에서 발생하는 패킷의 `분포를 분석`하여, 특정 유형의 패킷이 네트워크에서 얼마나 많이 발생하는지 추정할 수 있다.

### 적률의 특이 케이스

- **0번째 모멘트 (0th Moment)**:
  - 0th 모멘트는 고유한 요소의 개수를 의미한다.
  - 즉, 스트림에서 발견된 `서로 다른 요소의 수`를 측정한다. 이를 통해 데이터의 `다양성`을 파악할 수 있다.

- **1번째 모멘트 (1st Moment)**:
  - 1st 모멘트는 `요소의 총 개수`를 나타낸다.
  - 스트림에 있는 모든 요소의 개수를 의미하며, 이는 `스트림의 길이`와 같다.
  - 스트림에서 발생한 요소의 총 수를 계산한다.

- **2번째 모멘트 (2nd Moment)**:
  - 2nd 모멘트는 Surprise Number S라고 하며, 데이터의 `분포`가 얼마나 `고르지 않은지를 측정`하는 지표이다.
  - 이는 각 요소의 발생 빈도의 제곱을 합산한 값으로, 분포의 편차 또는 불균형 정도를 나타낸다.
  - 2nd 모멘트가 클수록 특정 요소가 더 많이 발생하고 있음을 의미한다.

- 이 모멘트 개념은 스트림의 요소에 대한 통계적 정보를 제공하며, 데이터 분석에 유용하다.

#### 예시: 2번째 모멘트 (Second Moment)

- **스트림 길이**: 100
- **고유 값의 수**: 11

##### 아이템 카운트 예시 1:
- 아이템 카운트: 10, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9
- Surprise S = 910 (계산: \(10^2 + 10 \times 9^2\))

##### 아이템 카운트 예시 2:
- 아이템 카운트: 90, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
- Surprise S = 8,110 (계산: \(90^2 + 10 \times 1^2\))

## AMS 방법: 모든 모멘트에 적용 가능!
- 하지만, 2번째 모멘트 S를 중점적으로 볼 것이다.

#### 여러 변수 X를 선택하고 추적
- 각 변수 X에 대해 X.el과 X.val을 저장한다.
  - `X.el`은 아이템 i에 해당한다.
  - `X.val`은 아이템 i의 개수에 해당한다.
- `제약`: 메인 메모리에 카운트를 저장해야 하므로, X의 수는 제한된다.

#### 목표: 2번째 모멘트 S 계산
\[ S = \sum_i m_i^2 \]

#### 하나의 랜덤 변수 (X) 설정 방법
- 스트림의 길이를 n이라고 가정하자 (나중에 이 가정을 완화할 것이다).
- 임의의 시간 t (t < n)를 선택하여 시작한다. 이때 ,시간은 동일한 확률로 선택된다.
- 시간 t에서 스트림에 아이템 i가 있다고 가정한다. 이때 `X.el` = i로 놓자.
- 그 후, 선택된 시간 t부터 스트림에서 아이템 i의 개수 c를 유지한다 (`X.val` = c).

-  정확한 2차 모멘트를 계산하는것은 m^2의 합이므로, m이 매우 크다면 메모리를 많이 사용해야한다. 이를 해결하기 위해 추정치를 사용한다.
#### 2번째 모멘트 추정치 계산
\[ S = f(X) = n (2 \cdot c - 1) \]

- 여러 개의 X (X1, X2, ..., Xk)를 추적할 것이다.
- 최종 추정치는 다음과 같이 계산된다:
\[ S = \frac{1}{k} \sum_{j=1}^{k} f(X_j) \]


#### 말로는 이해가 어렵다. 예시를 통해 이해해보자.

### 예시를 통해 AMS 방법 이해하기

![alt text](image-24.png)

이 예시는 AMS 방법이 어떻게 작동하는지, 그리고 추정치가 실제 값과 어떻게 비교되는지를 보여준다.

![alt text](image-25.png)

- 각 문자가 시간 `t = 1`부터 `t = 15`까지 순차적으로 나타납니다.

## 2. AMS 알고리즘 단계

### 1. 랜덤한 시간 선택
- `X1`, `X2`, `X3` 각각에 대해 스트림에서 랜덤한 시간을 하나씩 선택합니다. 이 시간이 해당 변수를 업데이트할 시점을 결정합니다.

### 2. 스트림에서 랜덤한 요소 추적

#### X1 (랜덤 시간 = 1):
- **초기화**: 시간 `1`에서 선택된 요소는 `c`, 초기 값(`X1.val`)은 `1`.
- **업데이트**:
  - 시간 `7`에서 `c`가 다시 등장하여 값이 `2`로 업데이트됩니다.
  - 4번째, 5번째, 6번째 시간에는 업데이트가 없습니다.
- **최종 값**: `X1.val = 2`.

#### X2 (랜덤 시간 = 8):
- **초기화**: 시간 `8`에서 선택된 요소는 `d`, 초기 값(`X2.val`)은 `1`.
- **업데이트**:
  - 시간 `11`에서 `d`가 다시 등장하여 값이 `2`로 업데이트됩니다.
  - 시간 `12`에서 `d`가 다시 등장하여 값이 `3`으로 업데이트됩니다.
  - 시간 `9`와 `10`에는 업데이트가 없습니다.
- **최종 값**: `X2.val = 3`.

#### X3 (랜덤 시간 = 13):
- **초기화**: 시간 `13`에서 선택된 요소는 `a`, 초기 값(`X3.val`)은 `1`.
- **업데이트**:
  - 시간 `14`에서 `a`가 다시 등장하여 값이 `2`로 업데이트됩니다.
  - 시간 `15`에는 업데이트가 없습니다.
- **최종 값**: `X3.val = 2`.

## 3. 두 번째 모멘트 계산

두 번째 모멘트 \( F_2 \)는 다음과 같은 공식으로 계산됩니다:
\[
F_2 = n \times (2 \cdot c - 1)
\]
- 여기서 \( n \)은 스트림에서 해당 요소가 등장한 총 횟수이며, \( c \)는 각 변수의 최종 값(`X.val`)입니다.

### X1에 대해:
- \( n = 15 \)
- \( X1.val = 2 \)
- \( F_2 = 15 \times (2 \times 2 - 1) = 75 \)

### X2에 대해:
- \( n = 15 \)
- \( X2.val = 3 \)
- \( F_2 = 15 \times (2 \times 3 - 1) = 45 \)

### X3에 대해:
- \( n = 15 \)
- \( X3.val = 2 \)
- \( F_2 = 15 \times (2 \times 2 - 1) = 45 \)

## 4. 최종 추정
최종 두 번째 모멘트 추정치는 세 개의 계산 값을 평균낸 것입니다:
\[
\frac{1}{3} \times (75 + 45 + 45) = 55
\]
실제 두 번째 모멘트는 `59`이며, 이 값과 `55`는 비교적 근접한 추정치입니다.

---

## 요약:
`AMS 알고리즘은 스트림에서 랜덤한 시간에 특정 요소를 선택하여 해당 요소의 빈도를 추적합니다. 이를 통해 스트림의 두 번째 모멘트(분산)를 추정할 수 있습니다. 여러 샘플의 결과를 평균 내어 최종 추정치를 계산합니다.`

- 각 문자가 시간 `t = 1`부터 `t = 15`까지 순서대로 등장한다.

---

# AMS(Alon-Matias-Szegedy) 알고리즘 및 기대값 분석

## 1. 스트림
`a, b, c, b, d, a, b, d, c, a, a, b, d, c, a, b`

- 각 문자가 시간 `t = 1`부터 `t = 15`까지 순서대로 등장한다.

---

## 2. AMS 알고리즘 과정

### 1. 랜덤한 시간 선택
- `X1`, `X2`, `X3`는 각각 랜덤한 시간을 선택하고, 그 시점에 등장한 요소를 추적한다.

### 2. 스트림에서 선택된 요소 추적

#### X1 (랜덤 시간 = 1):
- **초기화**: 시간 `1`에 `c`가 등장하고, 초기 값(`X1.val`)은 `1`로 설정된다.
- **업데이트**:
  - 시간 `7`에 `c`가 다시 등장하면 값이 `2`로 업데이트된다.
  - 4번째, 5번째, 6번째 시간에는 업데이트가 없다.
- **최종 값**: `X1.val = 2`.

#### X2 (랜덤 시간 = 8):
- **초기화**: 시간 `8`에 `d`가 등장하고, 초기 값(`X2.val`)은 `1`로 설정된다.
- **업데이트**:
  - 시간 `11`에 `d`가 다시 등장하면 값이 `2`로 업데이트된다.
  - 시간 `12`에 `d`가 다시 등장하면 값이 `3`으로 업데이트된다.
  - 시간 `9`와 `10`에는 업데이트가 없다.
- **최종 값**: `X2.val = 3`.

#### X3 (랜덤 시간 = 13):
- **초기화**: 시간 `13`에 `a`가 등장하고, 초기 값(`X3.val`)은 `1`로 설정된다.
- **업데이트**:
  - 시간 `14`에 `a`가 다시 등장하면 값이 `2`로 업데이트된다.
  - 시간 `15`에는 더 이상 업데이트가 없다.
- **최종 값**: `X3.val = 2`.

---

## 3. 2번째 모멘트 계산

2번째 모멘트는 스트림에서 각 아이템의 등장 횟수를 제곱해 구할 수 있다.

### 1. 2번째 모멘트 \( S \)
- 2번째 모멘트 \( S \)는 이렇게 계산된다:
  \[ S = \sum_i m_i^2 \]
  - 여기서 \( m_i \)는 아이템 \( i \)가 등장한 총 횟수다.

### 2. 시간 t에서 등장 횟수 \( c_t \)
- \( c_t \)는 시간 \( t \) 이후로 해당 아이템이 등장하는 횟수를 의미한다. 예를 들어:
  - \( c_1 = m_a \)는 시간 1부터 끝까지 `a`가 등장하는 횟수다.
  - \( c_2 = m_a - 1 \)는 시간 2 이후로 `a`가 등장하는 횟수다.
  - \( c_3 = m_b \)는 시간 3 이후로 `b`가 등장하는 횟수다.

### 3. 기대값 \( E[f(X)] \)
- 기대값 \( E[f(X)] \)는 다음과 같이 계산된다:
  \[ E[f(X)] = \frac{1}{n} \sum_{t=1}^{n} n(2c_t - 1) \]
  - 이때 \( n \)은 스트림 전체 길이이고, \( c_t \)는 해당 아이템이 이후에 등장하는 횟수다.

  이를 확장하면:
  \[ E[f(X)] = \frac{1}{n} \sum_i n (1 + 3 + 5 + \cdots + 2m_i - 1) \]
  - 이 수식은 아이템 \( i \)가 등장한 시점에서의 값을 더한 것이다.

---

## 4. 등차수열의 합

기대값 계산에서 등장한 수열 \( 1 + 3 + 5 + \dots + 2m_i - 1 \)은 **홀수 등차수열**이다.

### 1. 등차수열의 합 공식
- 홀수 수열의 합은 다음과 같은 공식으로 계산된다:
  \[
  \sum_{i=1}^{m_i} (2i - 1)
  \]
  - 이 수식은 \( i = 1 \)부터 \( m_i \)번째까지의 홀수를 더한 것이다.

### 2. 계산 과정
- 이 수식을 전개하면:
  \[
  \sum_{i=1}^{m_i} (2i - 1) = \frac{m_i \{2 + (m_i - 1) 2\}}{2}
  \]
  - \( m_i \)는 아이템이 등장한 횟수이고, 첫 항이 2이고 이후 2씩 증가하는 수열의 합이다.
  - 이를 풀면 \( m_i^2 \)이 된다.

### 3. 결론
- 따라서 홀수차 수열의 합은 이렇게 표현된다:
  \[
  \sum_{i=1}^{m_i} (2i - 1) = m_i^2
  \]
  - 즉, 이 수열의 합은 등장 횟수 \( m_i \)의 제곱으로 구할 수 있다.

---

## 5. 최종 기대값 계산

이제 최종적으로 기대값 \( E[f(X)] \)는 이렇게 계산된다:
  \[
  E[f(X)] = \frac{1}{n} \sum_i n (m_i^2)
  \]
  - 여기서 \( n \)은 스트림 전체 길이이고, \( m_i^2 \)는 각 아이템의 등장 횟수의 제곱이다.

---

## 6. 결론: 2번째 모멘트

- 최종적으로 기대값 \( E[f(X)] \)는 스트림의 2번째 모멘트 \( S \)와 같다:
  \[
  E[f(X)] = \sum_i m_i^2 = S
  \]
  - 각 아이템의 등장 횟수 제곱을 모두 더한 값으로, 데이터 분포의 변동성을 의미한다.

---

## 요약

- 2번째 모멘트 \( S \)는 각 아이템의 등장 횟수의 제곱을 더해서 계산한다.
- 등차수열의 합을 이용해 \( c_t \) 값을 계산하고 기대값을 구한다.
- 최종적으로 2번째 모멘트는 데이터 분포의 변동성을 나타낸다.

## 고차 적률(Higher-Order Moments)
<details>
<summary> `k번째 모멘트를 추정하는 방법` </summary>
<div markdown="1">

###  k번째 모멘트를 추정하는 방법

AMS 알고리즘을 사용해 k번째 모멘트를 추정할 때는 적률의 차수에 맞는 추정식을 사용한다. k가 달라질수록 추정식이 바뀌며, 이를 통해 k번째 적률을 계산할 수 있다.

#### 예시:
- **k=2**일 때 (2차 모멘트):
  \[ n(2c - 1) \]
  여기서 \( c \)는 특정 시점에 스트림에서 아이템이 등장한 횟수로, 이는 \( X.val \)로 나타낸다.

- **k=3**일 때 (3차 모멘트):
  \[ n(3c^2 - 3c + 1) \]
  여기서 \( c \)는 마찬가지로 등장한 횟수를 의미한다.

#### 왜 추정식이 달라지는가?

##### k=2일 때:
- k=2일 때는 \( 1 + 3 + 5 + \dots + (2m_i - 1) \)와 같은 수열을 사용하며, 이는 \( m_i^2 \)로 계산된다.
  \[
  \sum_{c=1}^{m}(2c - 1) = m^2
  \]
  이를 다시 풀면 다음과 같다:
  \[
  2c - 1 = c^2 - (c-1)^2
  \]
  따라서, 2차 모멘트는 각 아이템의 등장 횟수를 제곱하여 계산된다.

##### k=3일 때:
- k=3일 때는 다음과 같은 추정식을 사용한다:
  \[
  c^3 - (c-1)^3 = 3c^2 - 3c + 1
  \]
  이는 3차 모멘트를 추정하는 과정이다. 3차 모멘트는 각 아이템의 등장 횟수에 세제곱을 적용해 계산된다.

##### 일반적으로 k번째 모멘트를 추정하는 방법

일반적으로, k번째 모멘트를 추정할 때는 다음 공식을 사용한다:
  \[
  n \left( c^k - (c-1)^k \right)
  \]
  여기서 \( c^k \)는 등장 횟수를 k번 곱한 값이다.

---

#### 요약

AMS 알고리즘은 2차 모멘트뿐만 아니라 3차 이상의 고차 모멘트도 추정할 수 있다. 각 차수에 맞는 추정식을 사용하여 k번째 모멘트를 계산하며, 일반적으로는 \( n(c^k - (c-1)^k) \) 공식을 사용한다.

</div>
</details>

---

### 현실에서...

- **f(X) = n(2c - 1)**을 메모리에 담을 수 있는 만큼 많은 변수 \( X \)에 대해 계산한다.
- 계산한 값들을 **그룹으로 묶어 평균**을 구한다.
- 여러 그룹의 평균 중 **중앙값(median of averages)**을 구한다.

#### 스트림은 끝나지 않는다 (Problem: Streams never end)

- 우리는 스트림의 크기, 즉 스트림에서의 위치 수를 나타내는 **n**이라는 숫자가 있다고 가정했다.
- 하지만 실제 데이터 스트림은 **끝이 없는 경우**가 많다. 즉, n은 고정된 값이 아니라 계속 증가하는 **변수**가 된다.
- => 따라서 n은 **지금까지 처리한 입력의 개수**로 생각할 수 있다.



### 끝없는 스트림에서의 해결 방법 (Streams Never End: Fixups)
  #### 1. 변수 X에 대해 n을 별도로 관리

- **문제점**: 변수 X는 데이터 스트림의 길이 n을 포함하고 있다. 그러나 스트림이 끝없이 이어지는 상황에서는 n이 점점 커지며, 메모리 사용량도 증가할 수 있다. 이로 인해 전체 데이터 스트림을 추적하는 것이 어렵게 된다.
- **해결책**: X에는 스트림에서 `특정 데이터`의 카운트만 보관하고, 스트림의 길이인 `n은 별도로 관리`한다.
  - n은 지금까지 스트림에서 처리된 데이터의 총 개수를 의미한다. 이는 데이터를 처리하면서 계속 증가하기 때문에, 이를 별도로 관리하는 것이 효율적이다.
  - 이 방법을 사용하면 메모리 내에서 각 아이템이 등장한 횟수만 기록할 수 있으며, 전체 스트림의 크기 n은 별도로 관리하여 메모리 사용을 최소화할 수 있다.

  #### 예시: AMS 알고리즘에서 2차 모멘트 계산

- 사용하는 식: \( f(X) = n(2c - 1) \)
  - 여기서 n은 스트림에서 본 데이터의 총 개수를 의미하고, c는 특정 시간에 등장한 횟수를 의미한다.
  - 중요한 점은 **카운트 c**는 메모리에 저장되지만, n은 별도로 관리된다는 것이다.

### 2. k개의 카운트만 저장할 수 있는 경우

- **문제점**: 실제로 `메모리가 제한된 환경`에서는 오직 k개의 카운트만 저장할 수 있다. 이 경우 시간이 지나면서 더 많은 데이터가 스트림으로 들어오면, 새로운 데이터를 저장하기 위해 기존에 저장된 데이터를 버려야 하는 상황이 발생한다.
- **목표**: 데이터 스트림의 시작 시간 t가 k/n의 확률로 선택되도록 하는 것이다. 즉, 시간이 지나면서 새로운 데이터가 들어와도, 공정하게 선택되도록 보장해야 한다.
- **해결책**: `고정 크기 샘플링(Fixed-size sampling)`
  - 이 방법은 고정된 개수(k개의 변수)만을 메모리에 저장하면서, 스트림의 새로운 데이터를 확률적으로 선택하고 기존 데이터를 대체한다.

  #### 해결 과정

1. **처음에 k개의 시간을 선택**:
   - 스트림이 시작되면 처음에 `k개의 시간`을 선택하여 각 시간에 해당하는 `k개의 변수`를 메모리에 저장한다. 즉, 스트림에서 처음 k개의 데이터는 무조건 메모리에 담긴다.
   - 예를 들어, \( X_1, X_2, \ldots, X_k \)와 같은 변수를 선택해, 각 변수에 스트림 초기에 등장한 데이터를 기록한다.
   - 여기서 k는 메모리에서 관리할 수 있는 `최대 변수 개수`를 의미한다.

2. **n번째 데이터 도착 시 (n > k)**:
   - n번째 데이터가 스트림에 도착했을 때, 메모리에는 `이미 k개의 데이터`가 저장되어 있다.
   - 이때, 새로운 데이터를 저장할지 말지를 결정하기 위해, `k/n의 확률`로 이 데이터를 선택한다. 즉, 데이터가 많아질수록 n이 커지고, 새로운 데이터를 선택할 확률은 점점 작아진다.
     - 예를 들어, 스트림에서 100번째 데이터가 도착했을 때, 이 데이터를 k개의 변수 중 하나로 선택할 확률은 k/100이다.
     - 이는 초기에 들어온 데이터는 높은 확률로 메모리에 저장되지만, 시간이 지나면서 새로 들어오는 데이터는 선택될 확률이 점점 낮아진다는 뜻이다.

3. **새로운 데이터가 선택된 경우**:
   - 만약 새로운 데이터가 `k/n의 확률`로 선택되었다면, 메모리 내에 저장된 기존 데이터 중 하나를 동일한 확률로 선택해 제거한다.
     - 예를 들어, 새로운 데이터가 선택되었다면, 기존에 저장된 \( X_1, X_2, \ldots, X_k \) 중 하나를 랜덤하게 선택하여 제거하고, 그 자리에 새로운 데이터를 저장한다.
     - 이렇게 하면 메모리에 저장된 `k개의 데이터`가 유지되면서, 데이터 스트림을 계속 샘플링할 수 있다.

---

<details> 
<summary> 부연 설명 </summary>
<div markdown="1">

이 과정은 데이터 스트림이 끝없이 이어지더라도 `**고정된 메모리 크기**`를 유지하면서 데이터를 샘플링할 수 있게 해준다. 특히, 다음과 같은 장점을 갖는다:

- **메모리 효율성**:
  - 처음에 k개의 데이터를 선택하고, 이후에는 무작위로 데이터를 교체함으로써, `고정된 메모리 크기` 내에서 무한히 이어지는 스트림을 처리할 수 있다.
  - 메모리 크기가 제한되어 있더라도, 효율적으로 데이터를 저장하고 처리할 수 있다.

- **공정한 샘플링**:
  - n번째 이후 도착한 데이터도 `일정 확률`로 선택될 수 있기 때문에, `모든 데이터가 공정한 선택의 기회를 가진다.` 이는 시간이 지나면서 새로운 데이터가 메모리에 들어올 확률이 점점 줄어들지만, 그 데이터도 여전히 선택될 가능성이 있음을 보장한다.
  - 즉, 초기 데이터만 저장되는 것이 아니라, `나중에 들어오는 데이터도 기회를 갖게 된다.`

- **불필요한 데이터 제거**:
  - 새로운 데이터를 저장하기 위해 기존 데이터를 제거해야 할 때, `동등한 확률`로 기존 데이터 중 하나를 제거하게 된다. 이렇게 함으로써 특정 데이터가 메모리에 오래 남아있을 가능성이 균등하게 분배된다.
  - 메모리에 오래된 데이터만 남지 않고, `지속적으로 최신 데이터를 포함한 다양한 샘플`이 유지될 수 있다.
  
#### 요약
이 방법은 끝없는 데이터 스트림을 처리할 때 `메모리 한계를 극복`하기 위한 `효율적인 샘플링 기법`이다. 중요한 점은, k개의 변수를 고정 크기로 유지하면서 스트림의 모든 데이터를 공정하게 샘플링할 수 있다는 것이다. 새로 들어오는 데이터는 k/n의 확률로 선택되고, 기존 데이터는 동등한 확률로 제거되어, 고정된 메모리 크기 내에서 데이터 스트림을 계속 처리할 수 있다.
</div>
</details>

---
# Finding frequent elements

![alt text](image-26.png)

## 아이템 집합 세기 (Counting Itemsets)

### 문제

스트림에서 주어진 범위(window) 내에 **s**번 이상 등장한 아이템이 무엇인지 찾는 문제.

### 해결책

각 아이템의 등장 여부를 **이진 스트림(binary stream)**으로 생각.
- 1 = 아이템이 존재함.
- 0 = 아이템이 존재하지 않음.

`DGIM` 알고리즘을 사용하여 모든 아이템에 대해 1의 개수를 추정.
- DGIM은 오래된 항목의 영향을 줄이면서도 스트림에서 1의 개수를 효과적으로 추정하는 알고리즘.

### 예시

스트림 내에서 아이템의 등장 여부를 나타내는 이진값(1과 0)이 나열되어 있습니다.
- 스트림에서 각 구간에 대해 DGIM 방식으로 버킷을 생성하고, 각 버킷은 해당 구간에서 1이 몇 번 등장했는지를 나타냅니다.
- 최근 아이템에 대해 작은 버킷이 생성되며, 버킷 크기가 커질수록 오래된 데이터를 포함하게 됩니다.
- 이 방식으로 아이템의 빈도를 근사적으로 추정할 수 있습니다.

### 확장 (Extensions)

이 기법은 자주 등장하는 **쌍(pair)**이나 더 큰 아이템 집합을 세는 데에도 적용 가능.
- 각 아이템에 대해 하나의 이진 스트림을 생성하고, 스트림에서 1의 빈도를 계산.

### 단점 (Drawbacks)

- 근사값이므로 정확하지 않음.
- 아이템 집합의 수가 매우 많아질 수 있음.
- 빈도만을 고려하고 최신성을 고려하지 않음.
  - 예를 들어, 최근에 더 많은 가중치를 부여하지 못하는 한계가 있음.

## 지수 감소 창 (Exponentially Decaying Windows)

### 문제

현재 가장 인기 있는 아이템(영화 등)을 찾는 문제.
- 최근 항목에 더 높은 `가중치`를 부여해야 함.
- 단순히 최근 N개의 데이터를 계산하지 않고, 전체 스트림을 평활하여 집계해야 함.

### 해결책

각 스트림 항목에 대해 가중치를 적용하며, 최신 항목에 더 높은 가중치를 부여하는 방식.
- 이를 통해 전체 스트림에서 최신 트렌드를 반영한 집계를 할 수 있음.

스트림이 \(a_1, a_2, a_3, \ldots\)로 주어졌을 때, 스트림의 합을 다음과 같이 계산:
\[ \sum_{i=1}^{t} a_i (1 - c)^{t - i} \]

여기서 \(c\)는 매우 작은 상수로 설정됨 (예: \(10^{-6}\) 또는 \(10^{-9}\)).

#### 새로운 항목 도착 시:

- 현재 합계를 \((1 - c)\)로 곱함.
- 새로운 값을 더함.

## 아이템 세기 (Counting Items) 예시

### 문제

아이템이 있는지 없는지(1 또는 0)로 계산하는 문제.

### 해결책

각 아이템의 등장 여부를 **이진 스트림(binary stream)**으로 상상.
- \(a_i\)가 아이템이면, \(\delta_i = 1\)일 때 아이템이 등장하고, \(\delta_i = 0\)일 때는 등장하지 않음.

#### 새로운 항목이 도착할 때:

- 모든 기존 카운트를 \((1 - c)\)로 곱하고 새 항목을 더함.

\((1 - c)\)는 아이템의 "가중치"를 나타냄. 최신 항목일수록 높은 가중치를 부여하고, 오래된 항목은 가중치가 감소.

## 슬라이딩 창 vs. 지수 감소 창 (Sliding Windows vs. Decaying Windows)

### 슬라이딩 창 (Sliding Window)

- 일정 크기의 고정된 창을 사용하여 최신 항목만 계산.
- 창에서 벗어난 항목은 완전히 무시.

### 지수 감소 창 (Decaying Window)

- 전체 스트림에서 최신 항목에 더 큰 가중치를 부여하지만, 오래된 항목도 일정 부분 영향을 미침.
- 시간이 지남에 따라 오래된 항목의 가중치는 점차 감소.

#### 수식 설명

모든 가중치의 합은 다음과 같음:
\[ \sum_{i=1}^{\infty} (1 - c)^t = \frac{1}{c} \]
이는 \(c\)가 작을수록 더 넓은 범위의 항목이 고려됨을 의미하며, 최신 항목이 더 큰 영향을 미침.

## 아이템 세기 (Counting Items) 예시 - 현재 인기 있는 영화 찾기

### 문제

현재 가장 인기 있는 영화를 어떻게 찾을 것인가?

### 해결책

각 영화에 대해 별도의 스트림을 상상.
- 스트림에 영화가 등장할 때마다 그 영화에 대해 1을 기록.
- 다른 영화가 등장할 때는 0을 기록.

영화의 인기도는 그 영화의 등장 횟수의 지수 감소 합으로 계산됨.
- 최신 항목일수록 높은 가중치를 부여받고, 오래된 항목은 점차 가중치가 감소.

**임계값(threshold)** 설정 (예: ½).
- 인기도가 임계값 이하로 떨어지면 더 이상 추적하지 않음.

### 구체적인 작동 방식 (When a New Item Arrives)

새 티켓(아이템)이 스트림에 도착하면:

- 현재 유지 중인 각 영화의 점수를 **(1 - c)**로 곱함.
- 새로운 티켓이 특정 영화 M에 해당할 때:
  - 영화 M에 대해 점수가 이미 있다면 그 점수에 1을 더함.
  - 영화 M에 대한 점수가 없다면 새로운 점수를 만들어 1로 초기화.
- 점수가 임계값 이하로 떨어진 영화는 추적에서 제외.